{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing libraries and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Importing torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# check if we have cuda installed\n",
    "if torch.cuda.is_available():\n",
    "    # to use GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('GPU is:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "combined_df = pd.read_csv(r'../../data/combined_data.csv', sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "combined_df = combined_df[combined_df['text'].notnull()].reset_index()[['text', 'sentiment']]\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get feature and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Prepare features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify features and target\n",
    "features = combined_df.text.values.tolist()\n",
    "target = combined_df.sentiment.values.tolist()\n",
    "\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize features \n",
    "MAX_LEN = 128\n",
    "tokenized_feature = tokenizer.batch_encode_plus(\n",
    "    # Sentences to encode\n",
    "    features, \n",
    "    # Add '[CLS]' and '[SEP]'\n",
    "    add_special_tokens = True,\n",
    "    # Add empty tokens if len(text)<MAX_LEN\n",
    "    padding = 'max_length',\n",
    "    # Truncate all sentences to max length\n",
    "    truncation=True,\n",
    "    # Set the maximum length\n",
    "    max_length = MAX_LEN, \n",
    "    # Return attention mask\n",
    "    return_attention_mask = True,\n",
    "    # Return pytorch tensors\n",
    "    return_tensors = 'pt'       \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Preparing target\n",
    "\n",
    "The target will be cloned from input_ids and probability of 15% masked changing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloning target from input_ids\n",
    "target = tokenized_feature['input_ids'].detach().clone()\n",
    "\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15% masked probability\n",
    "probability = 0.15\n",
    "\n",
    "# create random array of floats in equal dimension to input_ids\n",
    "rand = torch.rand(tokenized_feature['input_ids'].shape)\n",
    "\n",
    "# where the random array is less than 0.15, we set true\n",
    "# mask_arr = rand < probability\n",
    "# prevent placing mask token on special tokens\n",
    "# (tokenized_feature['input_ids'] != 101) * (tokenized_feature['input_ids'] != 102)\n",
    "\n",
    "mask_arr = (rand < probability) * (tokenized_feature['input_ids'] != 101) * (tokenized_feature['input_ids'] != 102)\n",
    "\n",
    "# create selection from mask_arr\n",
    "selection = torch.flatten((mask_arr[0]).nonzero()).tolist()\n",
    "selection\n",
    "\n",
    "# apply selection index to inputs.input_ids, adding MASK tokens\n",
    "tokenized_feature['input_ids'][0, selection] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_feature['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Add language embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from language_tokens import get_lang_tokens\n",
    "\n",
    "{ 'special_token': 0, 'english': 1, 'malay': 2, 'other': 3 }\n",
    "\n",
    "language_ids = []\n",
    "for input_id in tokenized_feature['input_ids']:\n",
    "    input_tokens = input_id.tolist()\n",
    "    \n",
    "    language_ids.append(get_lang_tokens(tokenizer.batch_decode(input_tokens)))\n",
    "\n",
    "language_ids = torch.tensor(language_ids)\n",
    "\n",
    "tokenized_feature['language_ids'] = language_ids\n",
    "\n",
    "tokenized_feature['language_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Prepare train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 80% for training, 20% for testing and 20% for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, test_inputs, train_labels, test_labels, train_masks, test_masks, train_langs, test_langs, train_target, _ = train_test_split(tokenized_feature['input_ids'], \n",
    "                                                                                                                                                  target_num,\n",
    "                                                                                                                                                  tokenized_feature['attention_mask'],\n",
    "                                                                                                                                                  tokenized_feature['language_ids'],\n",
    "                                                                                                                                                  target,\n",
    "                                                                                                                                                  random_state=42,\n",
    "                                                                                                                                                  test_size=0.2,\n",
    "                                                                                                                                                  stratify=target)\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks, train_langs, validation_langs = train_test_split(train_inputs, \n",
    "                                                                                                                                                  train_labels,\n",
    "                                                                                                                                                  train_masks,\n",
    "                                                                                                                                                  train_langs,\n",
    "                                                                                                                                                  random_state=42,\n",
    "                                                                                                                                                  test_size=0.25,\n",
    "                                                                                                                                                  stratify=train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# define batch_size\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_langs, torch.tensor(train_labels))\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# Create the DataLoader for our validation set\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_langs,  torch.tensor(validation_labels))\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "# Create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_langs, torch.tensor(test_labels))\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Settings up BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_gpt2 import GPT2ForSequenceClassification, GPT2LMHeadModel\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    # Specify number of classes\n",
    "    num_labels = len(set(target)), \n",
    "    # Whether the model returns attentions weights\n",
    "    output_attentions = False,\n",
    "    # Whether the model returns all hidden-states \n",
    "    output_hidden_states = False\n",
    ")\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Optimizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Because we add two words [‘covid’, ‘coronavirus’] into the vocabulary\n",
    "# we will need to resize the token to make sure the model pick it up as whole words.\n",
    "\n",
    "# Receive the full size of the new word\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Optimizer & Learning Rate Scheduler\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "epochs = 20\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set a seed value.\n",
    "seed_val = 1024\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the average loss after each epoch \n",
    "loss_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Training\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "# number of total steps for each epoch\n",
    "print('total steps per epoch: ',  len(train_dataloader) / batch_size)\n",
    "# looping over epochs\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ====== Training ======\n",
    "    print('training on epoch: ', epoch_i)\n",
    "    progress_bar_train =  tqdm(range(len(train_dataloader)))\n",
    "    # set start time \n",
    "    t0 = time.time()\n",
    "    # reset total loss\n",
    "    total_loss = 0\n",
    "    # model in training \n",
    "    model.train()\n",
    "    # loop through batch \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # load data from dataloader \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_input_langs = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        # clear any previously calculated gradients \n",
    "        model.zero_grad()\n",
    "        # get outputs\n",
    "        outputs = model(input_ids=b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        language_ids=b_input_langs,\n",
    "                        labels=b_labels)\n",
    "        # get loss\n",
    "        loss = outputs[0]\n",
    "        # total loss\n",
    "        total_loss += loss.item()\n",
    "        # perform backward pass\n",
    "        # loss.backward()\n",
    "        # clip the norm of the gradients to 1.0.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update optimizer\n",
    "        optimizer.step()\n",
    "        # update learning rate \n",
    "        scheduler.step()\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar_train.update(1)\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    # ====== Validating ======\n",
    "    print('validating on epoch: ', epoch_i)\n",
    "    progress_bar_eval =  tqdm(range(len(validation_dataloader)))\n",
    "\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    epoch_acc_scores_list = []\n",
    "    targets_list = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "      # load data from dataloader \n",
    "      b_input_ids = batch[0].to(device)\n",
    "      b_input_mask = batch[1].to(device)\n",
    "      b_input_langs = batch[2].to(device)\n",
    "      b_labels = batch[3].to(device)\n",
    "\n",
    "      outputs = model(b_input_ids, \n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    language_ids=b_input_langs,\n",
    "                    labels=b_labels)\n",
    "      \n",
    "      loss = outputs[0]\n",
    "      total_loss += loss.item()\n",
    "      \n",
    "      progress_bar_eval.update(1)\n",
    "\n",
    "      # Get the preds\n",
    "      preds = outputs[1]\n",
    "      # Move preds to the CPU\n",
    "      val_preds = preds.detach().cpu().numpy()\n",
    "      # Move the labels to the cpu\n",
    "      targets_np = b_labels.to('cpu').numpy()\n",
    "      # Append the labels to a numpy list\n",
    "      targets_list.extend(targets_np)\n",
    "\n",
    "      if step == 0:  # first batch\n",
    "        stacked_val_preds = val_preds\n",
    "      else:\n",
    "        stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
    "\n",
    "      # Calculate the validation accuracy\n",
    "      y_true = targets_list\n",
    "      y_pred = np.argmax(stacked_val_preds, axis=1)\n",
    "\n",
    "      val_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "      epoch_acc_scores_list.append(val_acc)\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"average validation loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print('validation accuracy: ', val_acc)\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch_i,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'losses': loss_values\n",
    "    }, '/content/drive/MyDrive/model_state.bin')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "t0 = time.time()\n",
    "# model in validation mode\n",
    "model.eval()\n",
    "# save prediction\n",
    "losses = []\n",
    "acc = 0\n",
    "counter = 0\n",
    "# evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_input_langs, b_labels = batch\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        language_ids=b_input_langs,\n",
    "                        labels=b_labels)\n",
    "    # get output\n",
    "    loss = outputs[0]\n",
    "    logits = outputs[1]\n",
    "    # move logits and labels to CPU\n",
    "    _, prediction = torch.max(logits, dim=1)\n",
    "    labels = b_labels.cpu().detach().numpy().flatten()\n",
    "    prediction = np.argmax(logits, axis=-1).flatten()\n",
    "    accuracy = metrics.accuracy_score(labels, prediction)\n",
    "    \n",
    "    acc += accuracy\n",
    "    losses.append(loss.item())\n",
    "    counter += 1\n",
    "    \n",
    "print('total time used is: {0:.2f} s'.format(time.time() - t0))\n",
    "print('accuracy: {0:.2f}%'.format(acc / counter))\n",
    "print('losses: {0:.2f}%'.format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ..\\saved_model\\GPT2+LI\n",
    "\n",
    "torch.save(model, '../../saved_model/GPT2+LI/gpt2+li_mlm.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "!mkdir ..\\saved_model\\GPT2+LI\n",
    "    \n",
    "with open('../../saved_model/GPT2+LI/gpt2+li_mlm_predictions.bin', 'wb') as fp:\n",
    "    pickle.dump([losses, acc, counter], fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e07e2cad301f10046f31ca6b8439b04dc67a22fe5bd747bca8a9458062e70f77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
