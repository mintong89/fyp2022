{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing libraries and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Importing torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# check if we have cuda installed\n",
    "if torch.cuda.is_available():\n",
    "    # to use GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('GPU is:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "combined_df = pd.read_csv(r'../../data/combined_data.csv', sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "combined_df = combined_df[combined_df['text'].notnull()].reset_index()[['text', 'sentiment']]\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get feature and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Prepare features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['looks like scum kepada me', 'perhatian status aras air semasa di sarawak miri marudi aras air sungai adalah iait', 'change the heart of china kemudian', 'can rebate further dengan gc and shopback', 'aku baru habis tengok padman']\n"
     ]
    }
   ],
   "source": [
    "# identify features and target\n",
    "features = combined_df.text.values.tolist()\n",
    "target = combined_df.sentiment.values.tolist()\n",
    "\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize features \n",
    "MAX_LEN = 128\n",
    "tokenized_feature = tokenizer.batch_encode_plus(\n",
    "    # Sentences to encode\n",
    "    features, \n",
    "    # Add '[CLS]' and '[SEP]'\n",
    "    add_special_tokens = True,\n",
    "    # Add empty tokens if len(text)<MAX_LEN\n",
    "    padding = 'max_length',\n",
    "    # Truncate all sentences to max length\n",
    "    truncation=True,\n",
    "    # Set the maximum length\n",
    "    max_length = MAX_LEN, \n",
    "    # Return attention mask\n",
    "    return_attention_mask = True,\n",
    "    # Return pytorch tensors\n",
    "    return_tensors = 'pt'       \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Preparing target\n",
    "\n",
    "The target will be cloned from input_ids and probability of 15% masked changing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   101,  59148,  11850,  ...,      0,      0,      0],\n",
       "        [   101, 103601,  14042,  ...,      0,      0,      0],\n",
       "        [   101,  15453,  10105,  ...,      0,      0,      0],\n",
       "        ...,\n",
       "        [   101,  90567,  10592,  ...,      0,      0,      0],\n",
       "        [   101,  10659,  16113,  ...,      0,      0,      0],\n",
       "        [   101,  24604,  14657,  ...,      0,      0,      0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cloning target from input_ids\n",
    "target = tokenized_feature['input_ids'].detach().clone()\n",
    "\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15% masked probability\n",
    "probability = 0.15\n",
    "\n",
    "# create random array of floats in equal dimension to input_ids\n",
    "rand = torch.rand(tokenized_feature['input_ids'].shape)\n",
    "\n",
    "# where the random array is less than 0.15, we set true\n",
    "# mask_arr = rand < probability\n",
    "# prevent placing mask token on special tokens\n",
    "# (tokenized_feature['input_ids'] != 101) * (tokenized_feature['input_ids'] != 102)\n",
    "\n",
    "mask_arr = (rand < probability) * (tokenized_feature['input_ids'] != 101) * (tokenized_feature['input_ids'] != 102)\n",
    "\n",
    "# create selection from mask_arr\n",
    "selection = torch.flatten((mask_arr[0]).nonzero()).tolist()\n",
    "selection\n",
    "\n",
    "# apply selection index to inputs.input_ids, adding MASK tokens\n",
    "tokenized_feature['input_ids'][0, selection] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   101,  59148,  11850,  ...,      0,    103,      0],\n",
       "        [   101, 103601,  14042,  ...,      0,      0,      0],\n",
       "        [   101,  15453,  10105,  ...,      0,      0,      0],\n",
       "        ...,\n",
       "        [   101,  90567,  10592,  ...,      0,      0,      0],\n",
       "        [   101,  10659,  16113,  ...,      0,      0,      0],\n",
       "        [   101,  24604,  14657,  ...,      0,      0,      0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_feature['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Add language embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 2, 2, 0],\n",
       "        [0, 0, 0,  ..., 2, 2, 0],\n",
       "        [0, 0, 0,  ..., 2, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 2, 2, 0],\n",
       "        [0, 0, 0,  ..., 2, 2, 0],\n",
       "        [0, 0, 0,  ..., 2, 2, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from language_tokens import get_lang_tokens\n",
    "\n",
    "{ 'special_token': 0, 'english': 1, 'malay': 2, 'other': 3 }\n",
    "\n",
    "language_ids = []\n",
    "for input_id in tokenized_feature['input_ids']:\n",
    "    input_tokens = input_id.tolist()\n",
    "    \n",
    "    language_ids.append(get_lang_tokens(tokenizer.batch_decode(input_tokens)))\n",
    "\n",
    "language_ids = torch.tensor(language_ids)\n",
    "\n",
    "tokenized_feature['language_ids'] = language_ids\n",
    "\n",
    "tokenized_feature['language_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Prepare train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 80% for training and 20% for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks, train_langs, validation_langs = train_test_split(tokenized_feature['input_ids'], \n",
    "                                                                                                                                                  target,\n",
    "                                                                                                                                                  tokenized_feature['attention_mask'],\n",
    "                                                                                                                                                  tokenized_feature['language_ids'],\n",
    "                                                                                                                                                  random_state=2018,\n",
    "                                                                                                                                                  test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cafer\\AppData\\Local\\Temp\\ipykernel_14128\\2502411464.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = TensorDataset(train_inputs, train_masks, train_langs, torch.tensor(train_labels))\n",
      "C:\\Users\\cafer\\AppData\\Local\\Temp\\ipykernel_14128\\2502411464.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_data = TensorDataset(validation_inputs, validation_masks, validation_langs, torch.tensor(validation_labels))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler\n",
    "\n",
    "# define batch_size\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_langs, torch.tensor(train_labels))\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# Create the DataLoader for our test set\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_langs, torch.tensor(validation_labels))\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Settings up BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['bert.embeddings.language_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from modeling_gpt2 import GPT2ForSequenceClassification, GPT2LMHeadModel\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    # Specify number of classes\n",
    "    num_labels = len(set(target)), \n",
    "    # Whether the model returns attentions weights\n",
    "    output_attentions = False,\n",
    "    # Whether the model returns all hidden-states \n",
    "    output_hidden_states = False\n",
    ")\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Optimizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cafer\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Because we add two words [‘covid’, ‘coronavirus’] into the vocabulary\n",
    "# we will need to resize the token to make sure the model pick it up as whole words.\n",
    "\n",
    "# Receive the full size of the new word\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Optimizer & Learning Rate Scheduler\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "epochs = 20\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (language_embeddings): Embedding(119547, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use cuda if existing\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total steps per epoch:  156.25\n",
      "training on epoch:  0\n",
      "training on step:  50\n",
      "total time used is: 336.68 s\n",
      "training on step:  100\n",
      "total time used is: 676.86 s\n",
      "training on step:  150\n",
      "total time used is: 1019.46 s\n",
      "training on step:  200\n",
      "total time used is: 1352.80 s\n",
      "training on step:  250\n",
      "total time used is: 1685.69 s\n",
      "training on step:  300\n",
      "total time used is: 2018.14 s\n",
      "training on step:  350\n",
      "total time used is: 2344.39 s\n",
      "training on step:  400\n",
      "total time used is: 2678.25 s\n",
      "training on step:  450\n",
      "total time used is: 3011.39 s\n",
      "training on step:  500\n",
      "total time used is: 3344.70 s\n",
      "training on step:  550\n",
      "total time used is: 3673.67 s\n",
      "training on step:  600\n",
      "total time used is: 4005.16 s\n",
      "training on step:  650\n",
      "total time used is: 4324.02 s\n",
      "training on step:  700\n",
      "total time used is: 4633.43 s\n",
      "training on step:  750\n",
      "total time used is: 4945.56 s\n",
      "training on step:  800\n",
      "total time used is: 5257.95 s\n",
      "training on step:  850\n",
      "total time used is: 5574.25 s\n",
      "training on step:  900\n",
      "total time used is: 5898.33 s\n",
      "training on step:  950\n",
      "total time used is: 6232.69 s\n",
      "training on step:  1000\n",
      "total time used is: 6569.53 s\n",
      "training on step:  1050\n",
      "total time used is: 6908.20 s\n",
      "training on step:  1100\n",
      "total time used is: 7242.81 s\n",
      "training on step:  1150\n",
      "total time used is: 7578.94 s\n",
      "training on step:  1200\n",
      "total time used is: 7916.03 s\n",
      "training on step:  1250\n",
      "total time used is: 8251.83 s\n",
      "training on step:  1300\n",
      "total time used is: 8584.40 s\n",
      "training on step:  1350\n",
      "total time used is: 8916.65 s\n",
      "training on step:  1400\n",
      "total time used is: 9254.04 s\n",
      "training on step:  1450\n",
      "total time used is: 9595.62 s\n",
      "training on step:  1500\n",
      "total time used is: 9958.56 s\n",
      "training on step:  1550\n",
      "total time used is: 10334.27 s\n",
      "training on step:  1600\n",
      "total time used is: 10693.48 s\n",
      "training on step:  1650\n",
      "total time used is: 11059.46 s\n",
      "training on step:  1700\n",
      "total time used is: 11387.37 s\n",
      "training on step:  1750\n",
      "total time used is: 11689.73 s\n",
      "training on step:  1800\n",
      "total time used is: 11999.46 s\n",
      "training on step:  1850\n",
      "total time used is: 12303.74 s\n",
      "training on step:  1900\n",
      "total time used is: 12620.73 s\n",
      "training on step:  1950\n",
      "total time used is: 12987.12 s\n",
      "training on step:  2000\n",
      "total time used is: 13353.54 s\n",
      "training on step:  2050\n",
      "total time used is: 13719.39 s\n",
      "training on step:  2100\n",
      "total time used is: 14090.12 s\n",
      "training on step:  2150\n",
      "total time used is: 14498.22 s\n",
      "training on step:  2200\n",
      "total time used is: 14873.76 s\n",
      "training on step:  2250\n",
      "total time used is: 15248.73 s\n",
      "training on step:  2300\n",
      "total time used is: 15616.73 s\n",
      "training on step:  2350\n",
      "total time used is: 15983.87 s\n",
      "training on step:  2400\n",
      "total time used is: 16377.71 s\n",
      "training on step:  2450\n",
      "total time used is: 16796.93 s\n",
      "average training loss: 15.92\n",
      "training on epoch:  1\n",
      "training on step:  50\n",
      "total time used is: 381.49 s\n",
      "training on step:  100\n",
      "total time used is: 759.76 s\n",
      "training on step:  150\n",
      "total time used is: 1133.73 s\n",
      "training on step:  200\n",
      "total time used is: 1512.12 s\n",
      "training on step:  250\n",
      "total time used is: 1885.88 s\n",
      "training on step:  300\n",
      "total time used is: 2260.45 s\n",
      "training on step:  350\n",
      "total time used is: 2632.41 s\n",
      "training on step:  400\n",
      "total time used is: 3002.81 s\n",
      "training on step:  450\n",
      "total time used is: 3383.34 s\n",
      "training on step:  500\n",
      "total time used is: 3773.24 s\n",
      "training on step:  550\n",
      "total time used is: 4162.66 s\n",
      "training on step:  600\n",
      "total time used is: 4536.97 s\n",
      "training on step:  650\n",
      "total time used is: 4896.56 s\n",
      "training on step:  700\n",
      "total time used is: 5249.33 s\n",
      "training on step:  750\n",
      "total time used is: 5603.25 s\n",
      "training on step:  800\n",
      "total time used is: 5952.18 s\n",
      "training on step:  850\n",
      "total time used is: 6297.58 s\n",
      "training on step:  900\n",
      "total time used is: 6601.16 s\n",
      "training on step:  950\n",
      "total time used is: 6918.03 s\n",
      "training on step:  1000\n",
      "total time used is: 7231.09 s\n",
      "training on step:  1050\n",
      "total time used is: 7543.37 s\n",
      "training on step:  1100\n",
      "total time used is: 7851.72 s\n",
      "training on step:  1150\n",
      "total time used is: 8161.60 s\n",
      "training on step:  1200\n",
      "total time used is: 8468.96 s\n",
      "training on step:  1250\n",
      "total time used is: 8783.13 s\n",
      "training on step:  1300\n",
      "total time used is: 9098.20 s\n",
      "training on step:  1350\n",
      "total time used is: 9417.21 s\n",
      "training on step:  1400\n",
      "total time used is: 9731.60 s\n",
      "training on step:  1450\n",
      "total time used is: 10046.16 s\n",
      "training on step:  1500\n",
      "total time used is: 10358.39 s\n",
      "training on step:  1550\n",
      "total time used is: 10667.30 s\n",
      "training on step:  1600\n",
      "total time used is: 10975.39 s\n",
      "training on step:  1650\n",
      "total time used is: 11281.50 s\n",
      "training on step:  1700\n",
      "total time used is: 11591.09 s\n",
      "training on step:  1750\n",
      "total time used is: 11898.48 s\n",
      "training on step:  1800\n",
      "total time used is: 12210.03 s\n",
      "training on step:  1850\n",
      "total time used is: 12522.09 s\n",
      "training on step:  1900\n",
      "total time used is: 12825.60 s\n",
      "training on step:  1950\n",
      "total time used is: 13134.01 s\n",
      "training on step:  2000\n",
      "total time used is: 13444.50 s\n",
      "training on step:  2050\n",
      "total time used is: 13755.22 s\n",
      "training on step:  2100\n",
      "total time used is: 14060.47 s\n",
      "training on step:  2150\n",
      "total time used is: 14374.12 s\n",
      "training on step:  2200\n",
      "total time used is: 14684.76 s\n",
      "training on step:  2250\n",
      "total time used is: 14998.95 s\n",
      "training on step:  2300\n",
      "total time used is: 15315.28 s\n",
      "training on step:  2350\n",
      "total time used is: 15626.18 s\n",
      "training on step:  2400\n",
      "total time used is: 15937.73 s\n",
      "training on step:  2450\n",
      "total time used is: 16246.13 s\n",
      "average training loss: 15.92\n",
      "training on epoch:  2\n",
      "training on step:  50\n",
      "total time used is: 305.29 s\n",
      "training on step:  100\n",
      "total time used is: 615.26 s\n",
      "training on step:  150\n",
      "total time used is: 924.82 s\n",
      "training on step:  200\n",
      "total time used is: 1237.51 s\n",
      "training on step:  250\n",
      "total time used is: 1543.75 s\n",
      "training on step:  300\n",
      "total time used is: 1858.85 s\n",
      "training on step:  350\n",
      "total time used is: 2165.29 s\n",
      "training on step:  400\n",
      "total time used is: 2476.87 s\n",
      "training on step:  450\n",
      "total time used is: 2790.47 s\n",
      "training on step:  500\n",
      "total time used is: 3103.23 s\n",
      "training on step:  550\n",
      "total time used is: 3409.34 s\n",
      "training on step:  600\n",
      "total time used is: 3720.88 s\n",
      "training on step:  650\n",
      "total time used is: 4031.80 s\n",
      "training on step:  700\n",
      "total time used is: 4338.94 s\n",
      "training on step:  750\n",
      "total time used is: 4648.98 s\n",
      "training on step:  800\n",
      "total time used is: 4962.12 s\n",
      "training on step:  850\n",
      "total time used is: 5271.40 s\n",
      "training on step:  900\n",
      "total time used is: 5573.63 s\n",
      "training on step:  950\n",
      "total time used is: 5882.48 s\n",
      "training on step:  1000\n",
      "total time used is: 6192.13 s\n",
      "training on step:  1050\n",
      "total time used is: 6497.70 s\n",
      "training on step:  1100\n",
      "total time used is: 6801.36 s\n",
      "training on step:  1150\n",
      "total time used is: 7108.08 s\n",
      "training on step:  1200\n",
      "total time used is: 7417.13 s\n",
      "training on step:  1250\n",
      "total time used is: 7726.83 s\n",
      "training on step:  1300\n",
      "total time used is: 8037.03 s\n",
      "training on step:  1350\n",
      "total time used is: 8345.16 s\n",
      "training on step:  1400\n",
      "total time used is: 8647.64 s\n",
      "training on step:  1450\n",
      "total time used is: 8953.41 s\n",
      "training on step:  1500\n",
      "total time used is: 9265.32 s\n",
      "training on step:  1550\n",
      "total time used is: 9576.76 s\n",
      "training on step:  1600\n",
      "total time used is: 9915.52 s\n",
      "training on step:  1650\n",
      "total time used is: 10305.37 s\n",
      "training on step:  1700\n",
      "total time used is: 10657.18 s\n",
      "training on step:  1750\n",
      "total time used is: 11039.19 s\n",
      "training on step:  1800\n",
      "total time used is: 11379.12 s\n",
      "training on step:  1850\n",
      "total time used is: 11715.06 s\n",
      "training on step:  1900\n",
      "total time used is: 12047.67 s\n",
      "training on step:  1950\n",
      "total time used is: 12369.64 s\n",
      "training on step:  2000\n",
      "total time used is: 12690.75 s\n",
      "training on step:  2050\n",
      "total time used is: 13011.80 s\n",
      "training on step:  2100\n",
      "total time used is: 13332.56 s\n",
      "training on step:  2150\n",
      "total time used is: 13652.29 s\n",
      "training on step:  2200\n",
      "total time used is: 13971.51 s\n",
      "training on step:  2250\n",
      "total time used is: 14291.48 s\n",
      "training on step:  2300\n",
      "total time used is: 14610.22 s\n",
      "training on step:  2350\n",
      "total time used is: 14928.56 s\n",
      "training on step:  2400\n",
      "total time used is: 15248.44 s\n",
      "training on step:  2450\n",
      "total time used is: 15571.31 s\n",
      "average training loss: 15.92\n",
      "training on epoch:  3\n",
      "training on step:  50\n",
      "total time used is: 319.57 s\n",
      "training on step:  100\n",
      "total time used is: 643.51 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on step:  150\n",
      "total time used is: 969.11 s\n",
      "training on step:  200\n",
      "total time used is: 1289.35 s\n",
      "training on step:  250\n",
      "total time used is: 1605.61 s\n",
      "training on step:  300\n",
      "total time used is: 1923.60 s\n",
      "training on step:  350\n",
      "total time used is: 2241.73 s\n",
      "training on step:  400\n",
      "total time used is: 2560.01 s\n",
      "training on step:  450\n",
      "total time used is: 2877.87 s\n",
      "training on step:  500\n",
      "total time used is: 3196.69 s\n",
      "training on step:  550\n",
      "total time used is: 3513.20 s\n",
      "training on step:  600\n",
      "total time used is: 3830.38 s\n",
      "training on step:  650\n",
      "total time used is: 4147.81 s\n",
      "training on step:  700\n",
      "total time used is: 4467.68 s\n",
      "training on step:  750\n",
      "total time used is: 4787.31 s\n",
      "training on step:  800\n",
      "total time used is: 5110.69 s\n",
      "training on step:  850\n",
      "total time used is: 5431.79 s\n",
      "training on step:  900\n",
      "total time used is: 5752.04 s\n",
      "training on step:  950\n",
      "total time used is: 6088.04 s\n",
      "training on step:  1000\n",
      "total time used is: 6406.38 s\n",
      "training on step:  1050\n",
      "total time used is: 6736.09 s\n",
      "training on step:  1100\n",
      "total time used is: 21601.70 s\n",
      "training on step:  1150\n",
      "total time used is: 21910.15 s\n",
      "training on step:  1200\n",
      "total time used is: 22222.60 s\n",
      "training on step:  1250\n",
      "total time used is: 22533.89 s\n",
      "training on step:  1300\n",
      "total time used is: 22853.81 s\n",
      "training on step:  1350\n",
      "total time used is: 23182.05 s\n",
      "training on step:  1400\n",
      "total time used is: 23508.78 s\n",
      "training on step:  1450\n",
      "total time used is: 23835.64 s\n",
      "training on step:  1500\n",
      "total time used is: 24160.85 s\n",
      "training on step:  1550\n",
      "total time used is: 24495.95 s\n",
      "training on step:  1600\n",
      "total time used is: 24823.64 s\n",
      "training on step:  1650\n",
      "total time used is: 25150.65 s\n",
      "training on step:  1700\n",
      "total time used is: 25476.63 s\n",
      "training on step:  1750\n",
      "total time used is: 25807.18 s\n",
      "training on step:  1800\n",
      "total time used is: 26139.30 s\n",
      "training on step:  1850\n",
      "total time used is: 26462.50 s\n",
      "training on step:  1900\n",
      "total time used is: 26814.40 s\n",
      "training on step:  1950\n",
      "total time used is: 27162.44 s\n",
      "training on step:  2000\n",
      "total time used is: 27512.79 s\n",
      "training on step:  2050\n",
      "total time used is: 27842.43 s\n",
      "training on step:  2100\n",
      "total time used is: 28174.72 s\n",
      "training on step:  2150\n",
      "total time used is: 28512.74 s\n",
      "training on step:  2200\n",
      "total time used is: 28841.00 s\n",
      "training on step:  2250\n",
      "total time used is: 29172.37 s\n",
      "training on step:  2300\n",
      "total time used is: 29499.14 s\n",
      "training on step:  2350\n",
      "total time used is: 29826.98 s\n",
      "training on step:  2400\n",
      "total time used is: 30155.55 s\n",
      "training on step:  2450\n",
      "total time used is: 30484.40 s\n",
      "average training loss: 15.92\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import time\n",
    "# Store the average loss after each epoch \n",
    "loss_values = []\n",
    "# number of total steps for each epoch\n",
    "print('total steps per epoch: ',  len(train_dataloader) / batch_size)\n",
    "# looping over epochs\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    print('training on epoch: ', epoch_i)\n",
    "    # set start time \n",
    "    t0 = time.time()\n",
    "    # reset total loss\n",
    "    total_loss = 0\n",
    "    # model in training \n",
    "    model.train()\n",
    "    # loop through batch \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 50 step \n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('training on step: ', step)\n",
    "            print('total time used is: {0:.2f} s'.format(time.time() - t0))\n",
    "        # load data from dataloader \n",
    "        b_input_ids = batch[0].to(device).long()\n",
    "        b_input_mask = batch[1].to(device).long()\n",
    "        b_input_langs = batch[2].to(device).long()\n",
    "        b_labels = batch[3].to(device).long()\n",
    "        # clear any previously calculated gradients \n",
    "        model.zero_grad()\n",
    "        # get outputs\n",
    "        outputs = model(input_ids=b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        language_ids=b_input_langs,\n",
    "                        labels=b_labels)\n",
    "        # get loss\n",
    "        loss = outputs[0]\n",
    "        # total loss\n",
    "        total_loss += loss.item()\n",
    "        # clip the norm of the gradients to 1.0.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update optimizer\n",
    "        optimizer.step()\n",
    "        # update learning rate \n",
    "        scheduler.step()\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"average training loss: {0:.2f}\".format(avg_train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "t0 = time.time()\n",
    "# model in validation mode\n",
    "model.eval()\n",
    "# save prediction\n",
    "losses = []\n",
    "acc = 0\n",
    "counter = 0\n",
    "# evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_input_langs, b_labels = batch\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        language_ids=b_input_langs,\n",
    "                        labels=b_labels)\n",
    "    # get output\n",
    "    loss = outputs[0]\n",
    "    logits = outputs[1]\n",
    "    # move logits and labels to CPU\n",
    "    _, prediction = torch.max(logits, dim=1)\n",
    "    labels = b_labels.cpu().detach().numpy().flatten()\n",
    "    prediction = np.argmax(logits, axis=-1).flatten()\n",
    "    accuracy = metrics.accuracy_score(labels, prediction)\n",
    "    \n",
    "    acc += accuracy\n",
    "    losses.append(loss.item())\n",
    "    counter += 1\n",
    "    \n",
    "print('total time used is: {0:.2f} s'.format(time.time() - t0))\n",
    "print('accuracy: {0:.2f}%'.format(acc / counter))\n",
    "print('losses: {0:.2f}%'.format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file ..\\saved_model\\bert+li already exists.\n",
      "Error occurred while processing: ..\\saved_model\\bert+li.\n"
     ]
    }
   ],
   "source": [
    "!mkdir ..\\saved_model\\GPT2+LI\n",
    "\n",
    "torch.save(model, '../../saved_model/GPT2+LI/gpt2+li_mlm.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file ..\\saved_model\\bert+li already exists.\n",
      "Error occurred while processing: ..\\saved_model\\bert+li.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "!mkdir ..\\saved_model\\GPT2+LI\n",
    "    \n",
    "with open('../../saved_model/GPT2+LI/gpt2+li_mlm_predictions.bin', 'wb') as fp:\n",
    "    pickle.dump([losses, acc, counter], fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e07e2cad301f10046f31ca6b8439b04dc67a22fe5bd747bca8a9458062e70f77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
