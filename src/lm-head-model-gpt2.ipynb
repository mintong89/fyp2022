{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rHWZiZrKNvr",
        "outputId": "dc8f016e-27df-48da-c932-ec358f28aa6a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# set a seed value\n",
        "torch.manual_seed(555)\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfnrNVRPKNv5"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpGoTFB1KNv5"
      },
      "source": [
        "## 2.1. Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "KOW0AbsGYThF",
        "outputId": "629dd6f4-6fff-408a-8d4a-7a41ed8bca98"
      },
      "outputs": [],
      "source": [
        "sampled_df = pd.read_csv(r'sampled_data.csv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "df_train = sampled_df.groupby('sentiment').sample(n=40000).sample(frac=1)\n",
        "df_test = sampled_df.filter(items=list(filter(lambda x: x not in df_train.index, sampled_df.index)), axis=0)\n",
        "df_train = df_train.reset_index()[['text']]\n",
        "df_test = df_test.reset_index()[['text']]\n",
        "df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JIa0mAkKNv6"
      },
      "source": [
        "\n",
        "## 2.2. Create 10 Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCR804L3KNv6",
        "outputId": "08345023-e34e-4d69-bd19-5d2097e1296a"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "# shuffle\n",
        "df = shuffle(df_train)\n",
        "\n",
        "# initialize kfold\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=1024)\n",
        "\n",
        "# Note:\n",
        "# Each fold is a tuple ([train_index_values], [val_index_values])\n",
        "# fold_0, fold_1, fold_2, fold_3, fold_5 = kf.split(df, y)\n",
        "\n",
        "# Put the folds into a list. This is a list of tuples.\n",
        "fold_list = list(kf.split(df))\n",
        "\n",
        "train_df_list = []\n",
        "val_df_list = []\n",
        "\n",
        "for i, fold in enumerate(fold_list):\n",
        "\n",
        "    # map the train and val index values to dataframe rows\n",
        "    df_train = df[df.index.isin(fold[0])]\n",
        "    df_val = df[df.index.isin(fold[1])]\n",
        "    \n",
        "    train_df_list.append(df_train)\n",
        "    val_df_list.append(df_val)\n",
        "    \n",
        "    \n",
        "\n",
        "print(len(train_df_list))\n",
        "print(len(val_df_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67FLvqOoKNv7"
      },
      "source": [
        "# Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzoOOWhnKNv7"
      },
      "source": [
        "## 3.1. Train a Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo6HRmYJKNv8",
        "outputId": "e0dc04c9-e0d6-43b3-fd85-cf8a6c266324"
      },
      "outputs": [],
      "source": [
        "MODEL_TYPE = 'gpt2'\n",
        "\n",
        "NUM_FOLDS = 10\n",
        "NUM_FOLDS_TO_TRAIN = 5\n",
        "\n",
        "L_RATE = 2e-5\n",
        "MAX_LEN = 256\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 16\n",
        "NUM_CORES = os.cpu_count()\n",
        "\n",
        "NUM_CORES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9I9XQ05KNv8"
      },
      "source": [
        "## Instantiate the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33txAa3uD0FZ"
      },
      "outputs": [],
      "source": [
        "token_dict = {\n",
        "    'bos_token': '<|beginoftext|>',\n",
        "    'pad_token': '<|pad|>',\n",
        "    'sep_token': '<|sep|>',\n",
        "    'mask_token': '<|mask|>'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGijUa_wKNv8",
        "outputId": "b3559860-32ad-41cd-96b5-e930e5b43552"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load the tokenizer.\n",
        "print('Loading tokenizer...')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "    MODEL_TYPE,\n",
        "    do_lower_case = True\n",
        "    )\n",
        "\n",
        "tokenizer.add_special_tokens(token_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1yIGg5w_OdI",
        "outputId": "0abfa2ee-7acb-43cf-96ae-3cb6aefcca49"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode('<|pad|>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkknBn2sKNv8"
      },
      "source": [
        "## Create the Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxbAT37U7w4m",
        "outputId": "b41a4d60-f4d1-414b-e022-272f289d022f"
      },
      "outputs": [],
      "source": [
        "from language_tokens import get_lang_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEyvCmKbKNv8"
      },
      "outputs": [],
      "source": [
        "class CompDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df_data = df\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # get the sentence from the dataframe\n",
        "        features = self.df_data.loc[index, 'text']\n",
        "\n",
        "        # Process the sentence\n",
        "        # ---------------------\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                    features,           # Sentences to encode.\n",
        "                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
        "                    truncation = True,\n",
        "                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n",
        "                    pad_to_max_length = True,\n",
        "                    return_attention_mask = True,   # Construct attn. masks.\n",
        "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
        "               )  \n",
        "        \n",
        "        # These are torch tensors already.\n",
        "        input_ids = encoded_dict['input_ids'][0]\n",
        "        att_mask = encoded_dict['attention_mask'][0]\n",
        "        language_ids = torch.tensor(get_lang_tokens(\n",
        "            [x.replace(' ', '') for x in tokenizer.batch_decode(input_ids.tolist())]\n",
        "        ))\n",
        "\n",
        "        sample = (input_ids, att_mask, language_ids)\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df_data = df\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # get the sentence from the dataframe\n",
        "        features = self.df_data.loc[index, 'text']\n",
        "\n",
        "        # Process the sentence\n",
        "        # ---------------------\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                    features,           # Sentence to encode.\n",
        "                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
        "                    truncation = True,\n",
        "                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n",
        "                    pad_to_max_length = True,\n",
        "                    return_attention_mask = True,   # Construct attn. masks.\n",
        "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
        "               )\n",
        "        \n",
        "        # These are torch tensors already.\n",
        "        input_ids = encoded_dict['input_ids'][0]\n",
        "        att_mask = encoded_dict['attention_mask'][0]\n",
        "        language_ids = torch.tensor(get_lang_tokens(\n",
        "            [x.replace(' ', '') for x in tokenizer.batch_decode(input_ids.tolist())]\n",
        "        ))\n",
        "\n",
        "        sample = (input_ids, att_mask, language_ids)\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wicbcrP7KNv9"
      },
      "source": [
        "## Test the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "cwzDjEw2KNv9",
        "outputId": "07974e97-5623-4378-f512-f74b714e552e"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.reset_index(drop=True)\n",
        "df_val = df_val.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcVcVZ2ZKNv9",
        "outputId": "a827d8f4-dac8-4339-e2b4-816c98bbc27d"
      },
      "outputs": [],
      "source": [
        "train_data = CompDataset(df_train)\n",
        "val_data = CompDataset(df_val)\n",
        "test_data = TestDataset(df_test)\n",
        "\n",
        "\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True,\n",
        "                                       num_workers=NUM_CORES)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(val_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True,\n",
        "                                       num_workers=NUM_CORES)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=False,\n",
        "                                       num_workers=NUM_CORES)\n",
        "\n",
        "\n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(val_dataloader))\n",
        "print(len(test_dataloader))\n",
        "\n",
        "input_ids, att_mask, language_ids = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W9EAFyFVrLL",
        "outputId": "afb94ff8-f849-4e72-fd7a-abd2dc5a1c94"
      },
      "outputs": [],
      "source": [
        "input_ids[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iwctngbKNv9"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3VkN5AMRWBt",
        "outputId": "68865a17-e4ec-42b9-c1d9-e01e3bb650b7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# check if we have cuda installed\n",
        "if torch.cuda.is_available():\n",
        "    # to use GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('GPU is:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx9FDeWZKNv9",
        "outputId": "717b8d53-8ab0-4c8f-99e4-df25ab52258e"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Config\n",
        "from modeling_gpt2 import GPT2LMHeadModel\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "config = GPT2Config.from_pretrained(\n",
        "    MODEL_TYPE,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "    num_hidden_layers = 5,\n",
        "    num_attention_heads = 8,\n",
        "    hidden_dropout_prob = 0.2,\n",
        "    attention_probs_dropout_prob = 0.2,\n",
        "    ignore_mismatched_sizes=True,\n",
        "    bos_token_id = tokenizer.bos_token_id,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    eos_token_id = tokenizer.eos_token_id,\n",
        "    sep_token_id = tokenizer.sep_token_id\n",
        "    )\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\n",
        "    MODEL_TYPE,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Send the model to the device.\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD12jNzBvss-"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "  lr = L_RATE, \n",
        "  eps = 1e-8\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4V5cmgjKNwA"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjY-4ld-ceo3",
        "outputId": "3a168076-50ba-4b44-8411-60e829e6beca"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K06P9EM1DQoy"
      },
      "outputs": [],
      "source": [
        "# with open ('/content/drive/MyDrive/model.bin', 'rb') as fp:\n",
        "#   states = torch.load(fp)\n",
        "\n",
        "#   curr_epoch = states['epoch']\n",
        "#   model.load_state_dict(states['model'])\n",
        "#   optimizer.load_state_dict(states['optimizer'])\n",
        "\n",
        "#   del states\n",
        "\n",
        "# curr_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQw7-PznxAXk"
      },
      "outputs": [],
      "source": [
        "# initial settings\n",
        "\n",
        "curr_epoch = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587,
          "referenced_widgets": [
            "c6f56fc98dea45fc85363a15258b56e7",
            "ae11cdd9f8804fe2b37c39a79169f644",
            "7d21ee3297a4431d9e22bf8bbd9b456b",
            "8f9d452b17b2451ab1e02275b768fbef",
            "a3807a3973c04519a9bfaa96b0a9f59d",
            "8f6a5ca238f347fbaf5621d9b84965f6",
            "e342b33c8b4d41ab8e5d594519b906d1",
            "b2afc1e6bc414bdca6233f868f753b00",
            "16641c4426494c6e978ee5f1f65318d4",
            "fea570191c314fac85b8414d9f04822e",
            "7a332ab01161418da6706c856abe814d"
          ]
        },
        "id": "XibubGGrKNwA",
        "outputId": "a4e7f07e-05bb-41d6-f34e-81fe55a4201a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "import pickle\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set a seed value.\n",
        "seed_val = 1024\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "    \n",
        "\n",
        "# For each epoch...\n",
        "for epoch in range(curr_epoch, NUM_EPOCHS):\n",
        "    \n",
        "    print(\"\\nNum folds used for training:\", NUM_FOLDS_TO_TRAIN)\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n",
        "    \n",
        "    # Get the number of folds\n",
        "    num_folds = len(train_df_list)\n",
        "\n",
        "    # For this epoch, store the val acc scores for each fold in this list.\n",
        "    # We will use this list to calculate the cv at the end of the epoch.\n",
        "    epoch_acc_scores_list = []\n",
        "    \n",
        "    # For each fold...\n",
        "    for fold_index in range(1, NUM_FOLDS_TO_TRAIN):\n",
        "        \n",
        "        print('\\n== Fold Model', fold_index)\n",
        "\n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "        \n",
        "        stacked_val_labels = []\n",
        "        targets_list = []\n",
        "\n",
        "        print('Training...')\n",
        "\n",
        "        progress_bar_train =  tqdm(range(len(train_dataloader)))\n",
        "\n",
        "        # put the model into train mode\n",
        "        model.train()\n",
        "\n",
        "        # This turns gradient calculations on and off.\n",
        "        torch.set_grad_enabled(True)\n",
        "\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "            train_status = 'Batch ' + str(i+1) + ' of ' + str(len(train_dataloader))\n",
        "\n",
        "            print(train_status, end='\\r')\n",
        "\n",
        "\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_language_ids = batch[2].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(b_input_ids, \n",
        "                        attention_mask=b_input_mask,\n",
        "                        language_ids=b_language_ids,\n",
        "                        labels=b_input_ids)\n",
        "            \n",
        "            progress_bar_train.update(1)\n",
        "\n",
        "            # Get the loss from the outputs tuple: (loss, logits)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            # Convert the loss from a torch tensor to a number.\n",
        "            # Calculate the total loss.\n",
        "            total_train_loss = total_train_loss + loss.item()\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "            \n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Use the optimizer to update Weights\n",
        "            \n",
        "            # Optimizer for GPU\n",
        "            optimizer.step() \n",
        "            \n",
        "\n",
        "        print('Train loss:' ,total_train_loss)\n",
        "\n",
        "        # Save the Model\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }, '/content/drive/MyDrive/model.bin')\n",
        "        print('Saved model.')              \n",
        "        \n",
        "        # Use the garbage collector to save memory.\n",
        "        gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
    },
    "papermill": {
      "duration": 2795.593309,
      "end_time": "2020-08-19T07:29:53.573708",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-08-19T06:43:17.980399",
      "version": "2.1.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "e07e2cad301f10046f31ca6b8439b04dc67a22fe5bd747bca8a9458062e70f77"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16641c4426494c6e978ee5f1f65318d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a332ab01161418da6706c856abe814d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d21ee3297a4431d9e22bf8bbd9b456b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2afc1e6bc414bdca6233f868f753b00",
            "max": 6750,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16641c4426494c6e978ee5f1f65318d4",
            "value": 0
          }
        },
        "8f6a5ca238f347fbaf5621d9b84965f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f9d452b17b2451ab1e02275b768fbef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fea570191c314fac85b8414d9f04822e",
            "placeholder": "​",
            "style": "IPY_MODEL_7a332ab01161418da6706c856abe814d",
            "value": " 0/6750 [00:00&lt;?, ?it/s]"
          }
        },
        "a3807a3973c04519a9bfaa96b0a9f59d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae11cdd9f8804fe2b37c39a79169f644": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f6a5ca238f347fbaf5621d9b84965f6",
            "placeholder": "​",
            "style": "IPY_MODEL_e342b33c8b4d41ab8e5d594519b906d1",
            "value": "  0%"
          }
        },
        "b2afc1e6bc414bdca6233f868f753b00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6f56fc98dea45fc85363a15258b56e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae11cdd9f8804fe2b37c39a79169f644",
              "IPY_MODEL_7d21ee3297a4431d9e22bf8bbd9b456b",
              "IPY_MODEL_8f9d452b17b2451ab1e02275b768fbef"
            ],
            "layout": "IPY_MODEL_a3807a3973c04519a9bfaa96b0a9f59d"
          }
        },
        "e342b33c8b4d41ab8e5d594519b906d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fea570191c314fac85b8414d9f04822e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
