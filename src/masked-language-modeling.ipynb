{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rHWZiZrKNvr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# set a seed value\n",
        "torch.manual_seed(555)\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfnrNVRPKNv5"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpGoTFB1KNv5"
      },
      "source": [
        "## 2.1. Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOW0AbsGYThF"
      },
      "outputs": [],
      "source": [
        "sampled_df = pd.read_csv(r'sampled_data.csv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "df_train = sampled_df.groupby('sentiment').sample(n=40000).sample(frac=1)\n",
        "df_test = sampled_df.filter(items=list(filter(lambda x: x not in df_train.index, sampled_df.index)), axis=0)\n",
        "df_train = df_train.reset_index()[['text']]\n",
        "df_test = df_test.reset_index()[['text']]\n",
        "df_test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6JIa0mAkKNv6"
      },
      "source": [
        "\n",
        "## 2.2. Create Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCR804L3KNv6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "# shuffle\n",
        "df = shuffle(df_train)\n",
        "\n",
        "# initialize kfold\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=1024)\n",
        "\n",
        "# Note:\n",
        "# Each fold is a tuple ([train_index_values], [val_index_values])\n",
        "# fold_0, fold_1, fold_2, fold_3, fold_5 = kf.split(df, y)\n",
        "\n",
        "# Put the folds into a list. This is a list of tuples.\n",
        "fold_list = list(kf.split(df))\n",
        "\n",
        "train_df_list = []\n",
        "val_df_list = []\n",
        "\n",
        "for i, fold in enumerate(fold_list):\n",
        "\n",
        "    # map the train and val index values to dataframe rows\n",
        "    df_train = df[df.index.isin(fold[0])]\n",
        "    df_val = df[df.index.isin(fold[1])]\n",
        "    \n",
        "    train_df_list.append(df_train)\n",
        "    val_df_list.append(df_val)\n",
        "    \n",
        "    \n",
        "\n",
        "print(len(train_df_list))\n",
        "print(len(val_df_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67FLvqOoKNv7"
      },
      "source": [
        "# Section 3"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dzoOOWhnKNv7"
      },
      "source": [
        "## 3.1. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo6HRmYJKNv8"
      },
      "outputs": [],
      "source": [
        "MODEL_TYPE = 'xlm-roberta-base'\n",
        "\n",
        "NUM_FOLDS = 10\n",
        "NUM_FOLDS_TO_TRAIN = 5\n",
        "\n",
        "L_RATE = 2e-5\n",
        "MAX_LEN = 256\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 16\n",
        "NUM_CORES = os.cpu_count()\n",
        "\n",
        "NUM_CORES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9I9XQ05KNv8"
      },
      "source": [
        "## Instantiate the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGijUa_wKNv8"
      },
      "outputs": [],
      "source": [
        "from transformers import XLMRobertaTokenizer\n",
        "\n",
        "# Load the tokenizer.\n",
        "print('Loading tokenizer...')\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkknBn2sKNv8"
      },
      "source": [
        "## Create the Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxbAT37U7w4m"
      },
      "outputs": [],
      "source": [
        "from language_tokens import get_lang_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGKth1m5TajC"
      },
      "outputs": [],
      "source": [
        "probability = 0.15\n",
        "\n",
        "def to_masked_bert(input_ids):\n",
        "  result = input_ids\n",
        "  actual_tokens = list(set(range(100)) - \n",
        "                      set(np.where((result == 101) | (result == 102) \n",
        "                          | (result == 0))[0].tolist()))\n",
        "  \n",
        "  #We need to select 15% random tokens from the given list\n",
        "  num_of_token_to_mask = int(len(actual_tokens)*0.15)\n",
        "  token_to_mask = np.random.choice(np.array(actual_tokens), \n",
        "                                  size=num_of_token_to_mask, \n",
        "                                  replace=False).tolist()\n",
        "\n",
        "  #Now we have the indices where we need to mask the tokens\n",
        "  result[token_to_mask] = 103\n",
        "\n",
        "  return result\n",
        "\n",
        "def to_masked_xlmr(input_ids):\n",
        "  result = input_ids\n",
        "  actual_tokens = list(set(range(100)) - \n",
        "                      set(np.where((result == 0) | (result == 1) | \n",
        "                                   (result == 2) | (result == 3))[0].tolist()))\n",
        "  \n",
        "  #We need to select 15% random tokens from the given list\n",
        "  num_of_token_to_mask = int(len(actual_tokens)*0.15)\n",
        "  token_to_mask = np.random.choice(np.array(actual_tokens), \n",
        "                                  size=num_of_token_to_mask, \n",
        "                                  replace=False).tolist()\n",
        "\n",
        "  #Now we have the indices where we need to mask the tokens\n",
        "  result[token_to_mask] = 250001\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEyvCmKbKNv8"
      },
      "outputs": [],
      "source": [
        "class CompDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df_data = df\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # get the sentence from the dataframe\n",
        "        features = self.df_data.loc[index, 'text']\n",
        "\n",
        "        # Process the sentence\n",
        "        # ---------------------\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                    features,           # Sentences to encode.\n",
        "                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
        "                    truncation = True,\n",
        "                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n",
        "                    pad_to_max_length = True,\n",
        "                    return_attention_mask = True,   # Construct attn. masks.\n",
        "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
        "               )  \n",
        "        \n",
        "        # These are torch tensors already.\n",
        "        input_ids = encoded_dict['input_ids'][0]\n",
        "        att_mask = encoded_dict['attention_mask'][0]\n",
        "        # token_type_ids = encoded_dict['token_type_ids'][0]\n",
        "        \n",
        "        # clone input ids to target\n",
        "        target = input_ids.detach().clone()\n",
        "        # add mask token to input ids\n",
        "        input_ids = to_masked_xlmr(input_ids)\n",
        "\n",
        "        language_ids = torch.tensor(get_lang_tokens(\n",
        "            [x.replace(' ', '') for x in tokenizer.batch_decode(input_ids.tolist())]\n",
        "        ))\n",
        "\n",
        "        sample = (input_ids,\n",
        "                  att_mask,\n",
        "                  # token_type_ids,\n",
        "                  language_ids,\n",
        "                  target)\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df_data = df\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # get the sentence from the dataframe\n",
        "        features = self.df_data.loc[index, 'text']\n",
        "\n",
        "        # Process the sentence\n",
        "        # ---------------------\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                    features,           # Sentence to encode.\n",
        "                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
        "                    truncation = True,\n",
        "                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n",
        "                    pad_to_max_length = True,\n",
        "                    return_attention_mask = True,   # Construct attn. masks.\n",
        "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
        "               )\n",
        "        \n",
        "        # These are torch tensors already.\n",
        "        input_ids = encoded_dict['input_ids'][0]\n",
        "        att_mask = encoded_dict['attention_mask'][0]\n",
        "        token_type_ids = encoded_dict['token_type_ids'][0]\n",
        "        \n",
        "        # add mask token to input ids\n",
        "        input_ids = to_masked_xlmr(input_ids)\n",
        "\n",
        "        language_ids = torch.tensor(get_lang_tokens(\n",
        "            [x.replace(' ', '') for x in tokenizer.batch_decode(input_ids.tolist())]\n",
        "        ))\n",
        "\n",
        "        sample = (input_ids,\n",
        "                  att_mask,\n",
        "                  # token_type_ids,\n",
        "                  language_ids)\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wicbcrP7KNv9"
      },
      "source": [
        "## Test the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwzDjEw2KNv9"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.reset_index(drop=True)\n",
        "df_val = df_val.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcVcVZ2ZKNv9"
      },
      "outputs": [],
      "source": [
        "train_data = CompDataset(df_train)\n",
        "val_data = CompDataset(df_val)\n",
        "test_data = TestDataset(df_test)\n",
        "\n",
        "\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True,\n",
        "                                       num_workers=NUM_CORES)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(val_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True,\n",
        "                                       num_workers=NUM_CORES)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=False,\n",
        "                                       num_workers=NUM_CORES)\n",
        "\n",
        "\n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(val_dataloader))\n",
        "print(len(test_dataloader))\n",
        "\n",
        "input_ids, att_mask, language_ids, target = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iwctngbKNv9"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3VkN5AMRWBt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# check if we have cuda installed\n",
        "if torch.cuda.is_available():\n",
        "    # to use GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('GPU is:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx9FDeWZKNv9"
      },
      "outputs": [],
      "source": [
        "from transformers import XLMRobertaConfig\n",
        "from modeling_xlm_roberta import XLMRobertaForMaskedLM\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "config = XLMRobertaConfig.from_pretrained(\n",
        "    MODEL_TYPE,\n",
        "    num_labels = len(set([x.item() for x in target])),\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "    num_hidden_layers = 5,\n",
        "    num_attention_heads = 8,\n",
        "    hidden_dropout_prob = 0.2,\n",
        "    attention_probs_dropout_prob = 0.2,\n",
        "    ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "model = XLMRobertaForMaskedLM.from_pretrained(\n",
        "    MODEL_TYPE,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# Send the model to the device.\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD12jNzBvss-"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "  lr = L_RATE, \n",
        "  eps = 1e-8\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4V5cmgjKNwA"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjY-4ld-ceo3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K06P9EM1DQoy"
      },
      "outputs": [],
      "source": [
        "# with open ('/content/drive/MyDrive/model.bin', 'rb') as fp:\n",
        "#   states = torch.load(fp)\n",
        "\n",
        "#   curr_epoch = states['epoch']\n",
        "#   model.load_state_dict(states['model'])\n",
        "#   optimizer.load_state_dict(states['optimizer'])\n",
        "\n",
        "#   del states\n",
        "\n",
        "# curr_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQw7-PznxAXk"
      },
      "outputs": [],
      "source": [
        "# initial settings\n",
        "\n",
        "curr_epoch = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XibubGGrKNwA"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "import pickle\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set a seed value.\n",
        "seed_val = 1024\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "    \n",
        "\n",
        "# For each epoch...\n",
        "for epoch in range(curr_epoch, NUM_EPOCHS):\n",
        "    \n",
        "    print(\"\\nNum folds used for training:\", NUM_FOLDS_TO_TRAIN)\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n",
        "    \n",
        "    # Get the number of folds\n",
        "    num_folds = len(train_df_list)\n",
        "\n",
        "    # For this epoch, store the val acc scores for each fold in this list.\n",
        "    # We will use this list to calculate the cv at the end of the epoch.\n",
        "    epoch_acc_scores_list = []\n",
        "    \n",
        "    # For each fold...\n",
        "    for fold_index in range(1, NUM_FOLDS_TO_TRAIN):\n",
        "        \n",
        "        print('\\n== Fold Model', fold_index)\n",
        "\n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "        \n",
        "        stacked_val_labels = []\n",
        "        targets_list = []\n",
        "\n",
        "        print('Training...')\n",
        "\n",
        "        progress_bar_train =  tqdm(range(len(train_dataloader)))\n",
        "\n",
        "        # put the model into train mode\n",
        "        model.train()\n",
        "\n",
        "        # This turns gradient calculations on and off.\n",
        "        torch.set_grad_enabled(True)\n",
        "\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "            train_status = 'Batch ' + str(i+1) + ' of ' + str(len(train_dataloader))\n",
        "\n",
        "            print(train_status, end='\\r')\n",
        "\n",
        "\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            # b_token_type_ids = batch[2].to(device)\n",
        "            b_language_ids = batch[2].to(device)\n",
        "            b_labels = batch[3].to(device)\n",
        "\n",
        "            model.zero_grad()        \n",
        "\n",
        "\n",
        "            outputs = model(b_input_ids, \n",
        "                        # token_type_ids=b_token_type_ids, \n",
        "                        attention_mask=b_input_mask,\n",
        "                        language_ids=b_language_ids,\n",
        "                        labels=b_labels)\n",
        "            \n",
        "            progress_bar_train.update(1)\n",
        "\n",
        "            # Get the loss from the outputs tuple: (loss, logits)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            # Convert the loss from a torch tensor to a number.\n",
        "            # Calculate the total loss.\n",
        "            total_train_loss = total_train_loss + loss.item()\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "            \n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Use the optimizer to update Weights\n",
        "            \n",
        "            # Optimizer for GPU\n",
        "            optimizer.step() \n",
        "            \n",
        "        print('Train loss:' ,total_train_loss)\n",
        "\n",
        "        # Save the Model\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }, '/content/drive/MyDrive/model.bin')\n",
        "        print('Saved model.')              \n",
        "        \n",
        "        # Use the garbage collector to save memory.\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKyNEatWFXPB"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/final_model')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
    },
    "papermill": {
      "duration": 2795.593309,
      "end_time": "2020-08-19T07:29:53.573708",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-08-19T06:43:17.980399",
      "version": "2.1.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "e07e2cad301f10046f31ca6b8439b04dc67a22fe5bd747bca8a9458062e70f77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
