{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rHWZiZrKNvr",
        "outputId": "927204aa-0316-42b9-ba33-1ddb9a5c3d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "1.13.0+cu116\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# set a seed value\n",
        "torch.manual_seed(555)\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfnrNVRPKNv5"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpGoTFB1KNv5"
      },
      "source": [
        "## 2.1. Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc49LdYRa0oj"
      },
      "outputs": [],
      "source": [
        "sentiment_to_num = {\n",
        "    'Negative': 0,\n",
        "    'Neutral': 1,\n",
        "    'Positive': 2\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "KOW0AbsGYThF",
        "outputId": "789491d5-f4b1-4eca-c02b-729ff90064af"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-830c3719-b3d9-4c02-bc12-97ea09f6fe28\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>kenyataan media yb menteri kerajaan tempatan d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>berakhir sebelum memulai rasanya bagaimana sik</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wonder jika works untuk compressor also</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cut awak losses short since it might jadi the ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>samurai jin messy sia jangan want</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29995</th>\n",
              "      <td>after the successfully formed a planet</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29996</th>\n",
              "      <td>tidak hal abang naik moto itu jadi polis bantuan</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29997</th>\n",
              "      <td>kami di official facebook page pejabat pembang...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29998</th>\n",
              "      <td>- not banyak welfare and no satu dare kepada t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29999</th>\n",
              "      <td>the agency made a mistake debatable and they r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30000 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-830c3719-b3d9-4c02-bc12-97ea09f6fe28')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-830c3719-b3d9-4c02-bc12-97ea09f6fe28 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-830c3719-b3d9-4c02-bc12-97ea09f6fe28');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    text  sentiment\n",
              "0      kenyataan media yb menteri kerajaan tempatan d...          1\n",
              "1         berakhir sebelum memulai rasanya bagaimana sik          1\n",
              "2                wonder jika works untuk compressor also          0\n",
              "3      cut awak losses short since it might jadi the ...          2\n",
              "4                      samurai jin messy sia jangan want          0\n",
              "...                                                  ...        ...\n",
              "29995             after the successfully formed a planet          2\n",
              "29996   tidak hal abang naik moto itu jadi polis bantuan          0\n",
              "29997  kami di official facebook page pejabat pembang...          1\n",
              "29998  - not banyak welfare and no satu dare kepada t...          0\n",
              "29999  the agency made a mistake debatable and they r...          0\n",
              "\n",
              "[30000 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_df = pd.read_csv(r'sampled_data.csv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "df_train = sampled_df.groupby('sentiment').sample(n=40000).sample(frac=1)\n",
        "df_test = sampled_df.filter(items=list(filter(lambda x: x not in df_train.index, sampled_df.index)), axis=0)\n",
        "df_train = df_train.reset_index()[['text', 'sentiment']]\n",
        "df_test = df_test.reset_index()[['text', 'sentiment']]\n",
        "df_train['sentiment'] =  df_train['sentiment'].apply(lambda x: sentiment_to_num[x])\n",
        "df_test['sentiment'] =  df_test['sentiment'].apply(lambda x: sentiment_to_num[x])\n",
        "df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JIa0mAkKNv6"
      },
      "source": [
        "## 2.2. Create Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCR804L3KNv6",
        "outputId": "c090222a-b1f9-48a6-c66a-82b241991042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "# shuffle\n",
        "df = shuffle(df_train)\n",
        "\n",
        "# initialize kfold\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1024)\n",
        "\n",
        "# for stratification\n",
        "y = df['sentiment']\n",
        "\n",
        "# Note:\n",
        "# Each fold is a tuple ([train_index_values], [val_index_values])\n",
        "# fold_0, fold_1, fold_2, fold_3, fold_5 = kf.split(df, y)\n",
        "\n",
        "# Put the folds into a list. This is a list of tuples.\n",
        "fold_list = list(kf.split(df, y))\n",
        "\n",
        "train_df_list = []\n",
        "val_df_list = []\n",
        "\n",
        "for i, fold in enumerate(fold_list):\n",
        "\n",
        "    # map the train and val index values to dataframe rows\n",
        "    df_train = df[df.index.isin(fold[0])]\n",
        "    df_val = df[df.index.isin(fold[1])]\n",
        "    \n",
        "    train_df_list.append(df_train)\n",
        "    val_df_list.append(df_val)\n",
        "    \n",
        "    \n",
        "\n",
        "print(len(train_df_list))\n",
        "print(len(val_df_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67FLvqOoKNv7"
      },
      "source": [
        "# Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzoOOWhnKNv7"
      },
      "source": [
        "## 3.1. Train a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo6HRmYJKNv8",
        "outputId": "7a30f281-cfa6-47b2-f969-31e487684875"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL_TYPE = 'gpt2'\n",
        "\n",
        "NUM_FOLDS = 5\n",
        "\n",
        "# Saving 5 TPU models will exceed the 4.9GB disk space.\n",
        "# Therefore, will will only train on 3 folds.\n",
        "NUM_FOLDS_TO_TRAIN = 5 \n",
        "\n",
        "L_RATE = 2e-5\n",
        "MAX_LEN = 256\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "NUM_CORES = os.cpu_count()\n",
        "\n",
        "NUM_CORES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9I9XQ05KNv8"
      },
      "source": [
        "## Instantiate the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxOCRznb1mBz"
      },
      "outputs": [],
      "source": [
        "token_dict = {\n",
        "    'bos_token': '<|beginoftext|>',\n",
        "    'pad_token': '<|pad|>',\n",
        "    'sep_token': '<|sep|>',\n",
        "    'mask_token': '<|mask|>'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGijUa_wKNv8",
        "outputId": "97be4d74-113b-4314-c5b4-8c1c3db1ae94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer...\n",
            "397909\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load the tokenizer.\n",
        "print('Loading tokenizer...')\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(MODEL_TYPE, do_lower_case=True)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"tokenizer-gpt2/\", do_lower_case=True)\n",
        "tokenizer.add_special_tokens(token_dict)\n",
        "print(len(set(tokenizer.get_vocab().keys())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg9uCaLfiuB8"
      },
      "outputs": [],
      "source": [
        "# add vocabulary to tokenizer\n",
        "# ===========================\n",
        "\n",
        "# from tqdm.auto import tqdm\n",
        "# import nltk\n",
        "# nltk.download('words')\n",
        "\n",
        "# with open('combined-malay-dict.txt', encoding=\"utf8\") as fp:\n",
        "#   malay_words = set([x.strip() for x in fp.readlines()])\n",
        "\n",
        "# english_words = set(nltk.corpus.words.words())\n",
        "\n",
        "# new_tokens = (malay_words | english_words) - set(tokenizer.get_vocab().keys())\n",
        "\n",
        "# new_tokens = list(new_tokens)\n",
        "# batchsize = 10000\n",
        "# progress_bar =  tqdm(range(len(new_tokens)))\n",
        "\n",
        "# for i in range(0, len(new_tokens), batchsize):\n",
        "#   batch = new_tokens[i:i+batchsize]\n",
        "#   tokenizer.add_tokens(batch)\n",
        "#   progress_bar.update(batchsize)\n",
        "\n",
        "# tokenizer.add_special_tokens(token_dict)\n",
        "# tokenizer.save_pretrained(\"tokenizer/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkknBn2sKNv8"
      },
      "source": [
        "## Create the Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxbAT37U7w4m",
        "outputId": "84cf7eb7-ac99-4c47-d565-a6fe331c91b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from language_tokens import get_lang_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEyvCmKbKNv8"
      },
      "outputs": [],
      "source": [
        "class CompDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df_data = df\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # get the sentence from the dataframe\n",
        "        features = self.df_data.loc[index, 'text']\n",
        "\n",
        "        # Process the sentence\n",
        "        # ---------------------\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                    features,           # Sentences to encode.\n",
        "                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
        "                    truncation = True,\n",
        "                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n",
        "                    pad_to_max_length = True,\n",
        "                    return_attention_mask = True,   # Construct attn. masks.\n",
        "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
        "               )  \n",
        "        \n",
        "        # These are torch tensors already.\n",
        "        input_ids = encoded_dict['input_ids'][0]\n",
        "        att_mask = encoded_dict['attention_mask'][0]\n",
        "        language_ids = torch.tensor(get_lang_tokens(\n",
        "            [x.replace(' ', '') for x in tokenizer.batch_decode(input_ids.tolist())]\n",
        "            ))\n",
        "        \n",
        "        # Convert the target to a torch tensor\n",
        "        target = torch.tensor(self.df_data.loc[index, 'sentiment'])\n",
        "\n",
        "        sample = (input_ids, att_mask, language_ids, target)\n",
        "\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "class TestDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df_data = df\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # get the sentence from the dataframe\n",
        "        features = self.df_data.loc[index, 'text']\n",
        "\n",
        "        # Process the sentence\n",
        "        # ---------------------\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                    features,           # Sentence to encode.\n",
        "                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
        "                    truncation = True,\n",
        "                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n",
        "                    pad_to_max_length = True,\n",
        "                    return_attention_mask = True,   # Construct attn. masks.\n",
        "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
        "               )\n",
        "        \n",
        "        # These are torch tensors already.\n",
        "        input_ids = encoded_dict['input_ids'][0]\n",
        "        att_mask = encoded_dict['attention_mask'][0]\n",
        "        language_ids = torch.tensor(get_lang_tokens(\n",
        "            [x.replace(' ', '') for x in tokenizer.batch_decode(input_ids.tolist())]\n",
        "            ))\n",
        "               \n",
        "\n",
        "        sample = (input_ids, att_mask, language_ids)\n",
        "\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wicbcrP7KNv9"
      },
      "source": [
        "## Test the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "cwzDjEw2KNv9",
        "outputId": "1006e1a0-8dc1-46a9-d761-f8d073381fc7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6e921b65-ccb5-4d7b-96e2-f305e9bf3f7b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lim kim san toh chin chye</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>virgin - top of the list</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bukan kerajaan salah akak sebab akak lone rang...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hospital cyberjaya pun ada conveyor belt yang ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lima lokaliti di sabah dikenakan pkpd mulai</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23995</th>\n",
              "      <td>yang memburukkan islam sendiri adalah pas tung...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23996</th>\n",
              "      <td>looks like im doing this next</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23997</th>\n",
              "      <td>kamu semua boleh ada kepada cart kalau tak nak...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23998</th>\n",
              "      <td>jika awak know why lao goh purposely sayang st...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23999</th>\n",
              "      <td>masa bulan puasa hari itu before mula pkp</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e921b65-ccb5-4d7b-96e2-f305e9bf3f7b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6e921b65-ccb5-4d7b-96e2-f305e9bf3f7b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6e921b65-ccb5-4d7b-96e2-f305e9bf3f7b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    text  sentiment\n",
              "0                              lim kim san toh chin chye          1\n",
              "1                               virgin - top of the list          2\n",
              "2      bukan kerajaan salah akak sebab akak lone rang...          0\n",
              "3      hospital cyberjaya pun ada conveyor belt yang ...          1\n",
              "4            lima lokaliti di sabah dikenakan pkpd mulai          1\n",
              "...                                                  ...        ...\n",
              "23995  yang memburukkan islam sendiri adalah pas tung...          0\n",
              "23996                      looks like im doing this next          2\n",
              "23997  kamu semua boleh ada kepada cart kalau tak nak...          1\n",
              "23998  jika awak know why lao goh purposely sayang st...          2\n",
              "23999          masa bulan puasa hari itu before mula pkp          1\n",
              "\n",
              "[24000 rows x 2 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train = df_train.reset_index(drop=True)\n",
        "df_val = df_val.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcVcVZ2ZKNv9",
        "outputId": "db54ee37-0ef2-4768-9efc-4bee11f540c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1500\n",
            "375\n",
            "469\n"
          ]
        }
      ],
      "source": [
        "train_data = CompDataset(df_train)\n",
        "val_data = CompDataset(df_val)\n",
        "test_data = TestDataset(df_test)\n",
        "\n",
        "\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        # shuffle=True,\n",
        "                                       num_workers=NUM_CORES)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(val_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True,\n",
        "                                       num_workers=NUM_CORES)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=False,\n",
        "                                       num_workers=NUM_CORES)\n",
        "\n",
        "\n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(val_dataloader))\n",
        "print(len(test_dataloader))\n",
        "\n",
        "input_ids, att_mask, language_ids, target = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qQUwQYC9ZKFf",
        "outputId": "c137f4d2-b4a3-4765-ee9f-8f89adc620a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 4 torch.Size([64, 256])\n",
            "1 4 torch.Size([64, 256])\n",
            "2 4 torch.Size([64, 256])\n",
            "3 4 torch.Size([64, 256])\n",
            "4 4 torch.Size([64, 256])\n",
            "5 4 torch.Size([64, 256])\n",
            "6 4 torch.Size([64, 256])\n",
            "7 4 torch.Size([64, 256])\n",
            "8 4 torch.Size([64, 256])\n",
            "9 4 torch.Size([64, 256])\n",
            "10 4 torch.Size([64, 256])\n",
            "11 4 torch.Size([64, 256])\n",
            "12 4 torch.Size([64, 256])\n",
            "13 4 torch.Size([64, 256])\n",
            "14 4 torch.Size([64, 256])\n",
            "15 4 torch.Size([64, 256])\n",
            "16 4 torch.Size([64, 256])\n",
            "17 4 torch.Size([64, 256])\n",
            "18 4 torch.Size([64, 256])\n",
            "19 4 torch.Size([64, 256])\n",
            "20 4 torch.Size([64, 256])\n",
            "21 4 torch.Size([64, 256])\n",
            "22 4 torch.Size([64, 256])\n",
            "23 4 torch.Size([64, 256])\n",
            "24 4 torch.Size([64, 256])\n",
            "25 4 torch.Size([64, 256])\n",
            "26 4 torch.Size([64, 256])\n",
            "27 4 torch.Size([64, 256])\n",
            "28 4 torch.Size([64, 256])\n",
            "29 4 torch.Size([64, 256])\n",
            "30 4 torch.Size([64, 256])\n",
            "31 4 torch.Size([64, 256])\n",
            "32 4 torch.Size([64, 256])\n",
            "33 4 torch.Size([64, 256])\n",
            "34 4 torch.Size([64, 256])\n",
            "35 4 torch.Size([64, 256])\n",
            "36 4 torch.Size([64, 256])\n",
            "37 4 torch.Size([64, 256])\n",
            "38 4 torch.Size([64, 256])\n",
            "39 4 torch.Size([64, 256])\n",
            "40 4 torch.Size([64, 256])\n",
            "41 4 torch.Size([64, 256])\n",
            "42 4 torch.Size([64, 256])\n",
            "43 4 torch.Size([64, 256])\n",
            "44 4 torch.Size([64, 256])\n",
            "45 4 torch.Size([64, 256])\n",
            "46 4 torch.Size([64, 256])\n",
            "47 4 torch.Size([64, 256])\n",
            "48 4 torch.Size([64, 256])\n",
            "49 4 torch.Size([64, 256])\n",
            "50 4 torch.Size([64, 256])\n",
            "51 4 torch.Size([64, 256])\n",
            "52 4 torch.Size([64, 256])\n",
            "53 4 torch.Size([64, 256])\n",
            "54 4 torch.Size([64, 256])\n",
            "55 4 torch.Size([64, 256])\n",
            "56 4 torch.Size([64, 256])\n",
            "57 4 torch.Size([64, 256])\n",
            "58 4 torch.Size([64, 256])\n",
            "59 4 torch.Size([64, 256])\n",
            "60 4 torch.Size([64, 256])\n",
            "61 4 torch.Size([64, 256])\n",
            "62 4 torch.Size([64, 256])\n",
            "63 4 torch.Size([64, 256])\n",
            "64 4 torch.Size([64, 256])\n",
            "65 4 torch.Size([64, 256])\n",
            "66 4 torch.Size([64, 256])\n",
            "67 4 torch.Size([64, 256])\n",
            "68 4 torch.Size([64, 256])\n",
            "69 4 torch.Size([64, 256])\n",
            "70 4 torch.Size([64, 256])\n",
            "71 4 torch.Size([64, 256])\n",
            "72 4 torch.Size([64, 256])\n",
            "73 4 torch.Size([64, 256])\n",
            "74 4 torch.Size([64, 256])\n",
            "75 4 torch.Size([64, 256])\n",
            "76 4 torch.Size([64, 256])\n",
            "77 4 torch.Size([64, 256])\n",
            "78 4 torch.Size([64, 256])\n",
            "79 4 torch.Size([64, 256])\n",
            "80 4 torch.Size([64, 256])\n",
            "81 4 torch.Size([64, 256])\n",
            "82 4 torch.Size([64, 256])\n",
            "83 4 torch.Size([64, 256])\n",
            "84 4 torch.Size([64, 256])\n",
            "85 4 torch.Size([64, 256])\n",
            "86 4 torch.Size([64, 256])\n",
            "87 4 torch.Size([64, 256])\n",
            "88 4 torch.Size([64, 256])\n",
            "89 4 torch.Size([64, 256])\n",
            "90 4 torch.Size([64, 256])\n",
            "91 4 torch.Size([64, 256])\n",
            "92 4 torch.Size([64, 256])\n",
            "93 4 torch.Size([64, 256])\n",
            "94 4 torch.Size([64, 256])\n",
            "95 4 torch.Size([64, 256])\n",
            "96 4 torch.Size([64, 256])\n",
            "97 4 torch.Size([64, 256])\n",
            "98 4 torch.Size([64, 256])\n",
            "99 4 torch.Size([64, 256])\n",
            "100 4 torch.Size([64, 256])\n",
            "101 4 torch.Size([64, 256])\n",
            "102 4 torch.Size([64, 256])\n",
            "103 4 torch.Size([64, 256])\n",
            "104 4 torch.Size([64, 256])\n",
            "105 4 torch.Size([64, 256])\n",
            "106 4 torch.Size([64, 256])\n",
            "107 4 torch.Size([64, 256])\n",
            "108 4 torch.Size([64, 256])\n",
            "109 4 torch.Size([64, 256])\n",
            "110 4 torch.Size([64, 256])\n",
            "111 4 torch.Size([64, 256])\n",
            "112 4 torch.Size([64, 256])\n",
            "113 4 torch.Size([64, 256])\n",
            "114 4 torch.Size([64, 256])\n",
            "115 4 torch.Size([64, 256])\n",
            "116 4 torch.Size([64, 256])\n",
            "117 4 torch.Size([64, 256])\n",
            "118 4 torch.Size([64, 256])\n",
            "119 4 torch.Size([64, 256])\n",
            "120 4 torch.Size([64, 256])\n",
            "121 4 torch.Size([64, 256])\n",
            "122 4 torch.Size([64, 256])\n",
            "123 4 torch.Size([64, 256])\n",
            "124 4 torch.Size([64, 256])\n",
            "125 4 torch.Size([64, 256])\n",
            "126 4 torch.Size([64, 256])\n",
            "127 4 torch.Size([64, 256])\n",
            "128 4 torch.Size([64, 256])\n",
            "129 4 torch.Size([64, 256])\n",
            "130 4 torch.Size([64, 256])\n",
            "131 4 torch.Size([64, 256])\n",
            "132 4 torch.Size([64, 256])\n",
            "133 4 torch.Size([64, 256])\n",
            "134 4 torch.Size([64, 256])\n",
            "135 4 torch.Size([64, 256])\n",
            "136 4 torch.Size([64, 256])\n",
            "137 4 torch.Size([64, 256])\n",
            "138 4 torch.Size([64, 256])\n",
            "139 4 torch.Size([64, 256])\n",
            "140 4 torch.Size([64, 256])\n",
            "141 4 torch.Size([64, 256])\n",
            "142 4 torch.Size([64, 256])\n",
            "143 4 torch.Size([64, 256])\n",
            "144 4 torch.Size([64, 256])\n",
            "145 4 torch.Size([64, 256])\n",
            "146 4 torch.Size([64, 256])\n",
            "147 4 torch.Size([64, 256])\n",
            "148 4 torch.Size([64, 256])\n",
            "149 4 torch.Size([64, 256])\n",
            "150 4 torch.Size([64, 256])\n",
            "151 4 torch.Size([64, 256])\n",
            "152 4 torch.Size([64, 256])\n",
            "153 4 torch.Size([64, 256])\n",
            "154 4 torch.Size([64, 256])\n",
            "155 4 torch.Size([64, 256])\n",
            "156 4 torch.Size([64, 256])\n",
            "157 4 torch.Size([64, 256])\n",
            "158 4 torch.Size([64, 256])\n",
            "159 4 torch.Size([64, 256])\n",
            "160 4 torch.Size([64, 256])\n",
            "161 4 torch.Size([64, 256])\n",
            "162 4 torch.Size([64, 256])\n",
            "163 4 torch.Size([64, 256])\n",
            "164 4 torch.Size([64, 256])\n",
            "165 4 torch.Size([64, 256])\n",
            "166 4 torch.Size([64, 256])\n",
            "167 4 torch.Size([64, 256])\n",
            "168 4 torch.Size([64, 256])\n",
            "169 4 torch.Size([64, 256])\n",
            "170 4 torch.Size([64, 256])\n",
            "171 4 torch.Size([64, 256])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fab1d08f160>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
            "    self._shutdown_workers()\n",
            "Exception ignored in:   File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
            "    if w.is_alive():<function _MultiProcessingDataLoaderIter.__del__ at 0x7fab1d08f160>\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "        self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
            "    if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError\n",
            "AssertionError: : can only test a child process\n",
            "can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172 4 torch.Size([64, 256])\n",
            "173 4 torch.Size([64, 256])\n",
            "174 4 torch.Size([64, 256])\n",
            "175 4 torch.Size([64, 256])\n",
            "176 4 torch.Size([64, 256])\n",
            "177 4 torch.Size([64, 256])\n",
            "178 4 torch.Size([64, 256])\n",
            "179 4 torch.Size([64, 256])\n",
            "180 4 torch.Size([64, 256])\n",
            "181 4 torch.Size([64, 256])\n",
            "182 4 torch.Size([64, 256])\n",
            "183 4 torch.Size([64, 256])\n",
            "184 4 torch.Size([64, 256])\n",
            "185 4 torch.Size([64, 256])\n",
            "186 4 torch.Size([64, 256])\n",
            "187 4 torch.Size([64, 256])\n",
            "188 4 torch.Size([64, 256])\n",
            "189 4 torch.Size([64, 256])\n",
            "190 4 torch.Size([64, 256])\n",
            "191 4 torch.Size([64, 256])\n",
            "192 4 torch.Size([64, 256])\n",
            "193 4 torch.Size([64, 256])\n",
            "194 4 torch.Size([64, 256])\n",
            "195 4 torch.Size([64, 256])\n",
            "196 4 torch.Size([64, 256])\n",
            "197 4 torch.Size([64, 256])\n",
            "198 4 torch.Size([64, 256])\n",
            "199 4 torch.Size([64, 256])\n",
            "200 4 torch.Size([64, 256])\n",
            "201 4 torch.Size([64, 256])\n",
            "202 4 torch.Size([64, 256])\n",
            "203 4 torch.Size([64, 256])\n",
            "204 4 torch.Size([64, 256])\n",
            "205 4 torch.Size([64, 256])\n",
            "206 4 torch.Size([64, 256])\n",
            "207 4 torch.Size([64, 256])\n",
            "208 4 torch.Size([64, 256])\n",
            "209 4 torch.Size([64, 256])\n",
            "210 4 torch.Size([64, 256])\n",
            "211 4 torch.Size([64, 256])\n",
            "212 4 torch.Size([64, 256])\n",
            "213 4 torch.Size([64, 256])\n",
            "214 4 torch.Size([64, 256])\n",
            "215 4 torch.Size([64, 256])\n",
            "216 4 torch.Size([64, 256])\n",
            "217 4 torch.Size([64, 256])\n",
            "218 4 torch.Size([64, 256])\n",
            "219 4 torch.Size([64, 256])\n",
            "220 4 torch.Size([64, 256])\n",
            "221 4 torch.Size([64, 256])\n",
            "222 4 torch.Size([64, 256])\n",
            "223 4 torch.Size([64, 256])\n",
            "224 4 torch.Size([64, 256])\n",
            "225 4 torch.Size([64, 256])\n",
            "226 4 torch.Size([64, 256])\n",
            "227 4 torch.Size([64, 256])\n",
            "228 4 torch.Size([64, 256])\n",
            "229 4 torch.Size([64, 256])\n",
            "230 4 torch.Size([64, 256])\n",
            "231 4 torch.Size([64, 256])\n",
            "232 4 torch.Size([64, 256])\n",
            "233 4 torch.Size([64, 256])\n",
            "234 4 torch.Size([64, 256])\n",
            "235 4 torch.Size([64, 256])\n",
            "236 4 torch.Size([64, 256])\n",
            "237 4 torch.Size([64, 256])\n",
            "238 4 torch.Size([64, 256])\n",
            "239 4 torch.Size([64, 256])\n",
            "240 4 torch.Size([64, 256])\n",
            "241 4 torch.Size([64, 256])\n",
            "242 4 torch.Size([64, 256])\n",
            "243 4 torch.Size([64, 256])\n",
            "244 4 torch.Size([64, 256])\n",
            "245 4 torch.Size([64, 256])\n",
            "246 4 torch.Size([64, 256])\n",
            "247 4 torch.Size([64, 256])\n",
            "248 4 torch.Size([64, 256])\n",
            "249 4 torch.Size([64, 256])\n",
            "250 4 torch.Size([64, 256])\n",
            "251 4 torch.Size([64, 256])\n",
            "252 4 torch.Size([64, 256])\n",
            "253 4 torch.Size([64, 256])\n",
            "254 4 torch.Size([64, 256])\n",
            "255 4 torch.Size([64, 256])\n",
            "256 4 torch.Size([64, 256])\n",
            "257 4 torch.Size([64, 256])\n",
            "258 4 torch.Size([64, 256])\n",
            "259 4 torch.Size([64, 256])\n",
            "260 4 torch.Size([64, 256])\n",
            "261 4 torch.Size([64, 256])\n",
            "262 4 torch.Size([64, 256])\n",
            "263 4 torch.Size([64, 256])\n",
            "264 4 torch.Size([64, 256])\n",
            "265 4 torch.Size([64, 256])\n",
            "266 4 torch.Size([64, 256])\n",
            "267 4 torch.Size([64, 256])\n",
            "268 4 torch.Size([64, 256])\n",
            "269 4 torch.Size([64, 256])\n",
            "270 4 torch.Size([64, 256])\n",
            "271 4 torch.Size([64, 256])\n",
            "272 4 torch.Size([64, 256])\n",
            "273 4 torch.Size([64, 256])\n",
            "274 4 torch.Size([64, 256])\n",
            "275 4 torch.Size([64, 256])\n",
            "276 4 torch.Size([64, 256])\n",
            "277 4 torch.Size([64, 256])\n",
            "278 4 torch.Size([64, 256])\n",
            "279 4 torch.Size([64, 256])\n",
            "280 4 torch.Size([64, 256])\n",
            "281 4 torch.Size([64, 256])\n",
            "282 4 torch.Size([64, 256])\n",
            "283 4 torch.Size([64, 256])\n",
            "284 4 torch.Size([64, 256])\n",
            "285 4 torch.Size([64, 256])\n",
            "286 4 torch.Size([64, 256])\n",
            "287 4 torch.Size([64, 256])\n",
            "288 4 torch.Size([64, 256])\n",
            "289 4 torch.Size([64, 256])\n",
            "290 4 torch.Size([64, 256])\n",
            "291 4 torch.Size([64, 256])\n",
            "292 4 torch.Size([64, 256])\n",
            "293 4 torch.Size([64, 256])\n",
            "294 4 torch.Size([64, 256])\n",
            "295 4 torch.Size([64, 256])\n",
            "296 4 torch.Size([64, 256])\n",
            "297 4 torch.Size([64, 256])\n",
            "298 4 torch.Size([64, 256])\n",
            "299 4 torch.Size([64, 256])\n",
            "300 4 torch.Size([64, 256])\n",
            "301 4 torch.Size([64, 256])\n",
            "302 4 torch.Size([64, 256])\n",
            "303 4 torch.Size([64, 256])\n",
            "304 4 torch.Size([64, 256])\n",
            "305 4 torch.Size([64, 256])\n",
            "306 4 torch.Size([64, 256])\n",
            "307 4 torch.Size([64, 256])\n",
            "308 4 torch.Size([64, 256])\n",
            "309 4 torch.Size([64, 256])\n",
            "310 4 torch.Size([64, 256])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-4e2ef3550e44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
          ]
        }
      ],
      "source": [
        "for i, b in enumerate(train_dataloader):\n",
        "  print(i, len(b), b[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFqVMqhzZfvX",
        "outputId": "02245695-9071-4802-d3c3-b0e711a80400"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[torch.Size([256]), torch.Size([256]), torch.Size([256]), torch.Size([])]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[x.shape for x in train_data[311]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iwctngbKNv9"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3VkN5AMRWBt",
        "outputId": "0cd9acdf-5783-440f-c400-ad957c8d0343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "GPU is: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# check if we have cuda installed\n",
        "if torch.cuda.is_available():\n",
        "    # to use GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('GPU is:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx9FDeWZKNv9",
        "outputId": "7d37e156-17a4-47f0-9ee7-031fb64c7887"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at gpt2 were not used when initializing GPT2ForSequenceClassification: ['h.5.mlp.c_fc.weight', 'h.7.mlp.c_proj.weight', 'h.7.mlp.c_proj.bias', 'h.7.attn.c_attn.bias', 'h.5.ln_1.bias', 'h.5.attn.c_attn.weight', 'h.7.mlp.c_fc.weight', 'h.8.mlp.c_proj.bias', 'h.10.mlp.c_proj.weight', 'h.11.ln_2.weight', 'h.7.ln_2.weight', 'h.8.mlp.c_fc.weight', 'h.5.ln_2.weight', 'h.11.mlp.c_proj.bias', 'h.11.ln_1.weight', 'h.7.ln_1.bias', 'h.11.attn.c_attn.weight', 'h.6.ln_1.bias', 'h.10.mlp.c_fc.bias', 'h.6.attn.c_proj.weight', 'h.5.mlp.c_proj.weight', 'h.6.attn.c_attn.bias', 'h.10.ln_2.weight', 'h.5.ln_1.weight', 'h.7.mlp.c_fc.bias', 'h.9.mlp.c_proj.bias', 'h.7.attn.c_proj.weight', 'h.10.ln_1.bias', 'h.10.attn.c_attn.weight', 'h.6.mlp.c_fc.weight', 'h.11.mlp.c_fc.weight', 'h.11.attn.c_attn.bias', 'h.10.attn.c_proj.weight', 'h.5.mlp.c_proj.bias', 'h.10.attn.bias', 'h.11.attn.c_proj.bias', 'h.5.ln_2.bias', 'h.11.ln_2.bias', 'h.9.mlp.c_fc.bias', 'h.8.attn.c_proj.weight', 'h.6.attn.bias', 'h.11.mlp.c_proj.weight', 'h.8.mlp.c_fc.bias', 'h.5.attn.bias', 'h.11.mlp.c_fc.bias', 'h.10.attn.c_proj.bias', 'h.5.attn.c_proj.bias', 'h.8.ln_1.weight', 'h.8.mlp.c_proj.weight', 'h.9.ln_2.weight', 'h.7.attn.bias', 'h.9.mlp.c_proj.weight', 'h.6.mlp.c_proj.bias', 'h.7.attn.c_attn.weight', 'h.10.mlp.c_fc.weight', 'h.11.attn.bias', 'h.6.mlp.c_fc.bias', 'h.5.mlp.c_fc.bias', 'h.11.attn.c_proj.weight', 'h.7.ln_1.weight', 'h.8.ln_1.bias', 'h.6.ln_1.weight', 'h.6.ln_2.bias', 'h.5.attn.c_proj.weight', 'h.7.attn.c_proj.bias', 'h.8.ln_2.weight', 'h.6.attn.c_attn.weight', 'h.8.ln_2.bias', 'h.9.ln_1.bias', 'h.9.ln_2.bias', 'h.9.mlp.c_fc.weight', 'h.9.attn.c_attn.weight', 'h.10.mlp.c_proj.bias', 'h.7.ln_2.bias', 'h.9.attn.c_proj.weight', 'h.9.attn.c_proj.bias', 'h.9.attn.bias', 'h.10.ln_1.weight', 'h.8.attn.bias', 'h.10.ln_2.bias', 'h.9.attn.c_attn.bias', 'h.6.mlp.c_proj.weight', 'h.11.ln_1.bias', 'h.6.attn.c_proj.bias', 'h.8.attn.c_proj.bias', 'h.8.attn.c_attn.bias', 'h.6.ln_2.weight', 'h.9.ln_1.weight', 'h.8.attn.c_attn.weight', 'h.10.attn.c_attn.bias', 'h.5.attn.c_attn.bias']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['wle.weight', 'score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT2ForSequenceClassification(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50261, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (wle): Embedding(50257, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2Config\n",
        "from modeling_gpt2 import GPT2ForSequenceClassification\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "config = GPT2Config.from_pretrained(\n",
        "    MODEL_TYPE,\n",
        "    num_labels = len(set([x.item() for x in target])) ,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "    num_hidden_layers = 5,\n",
        "    num_attention_heads = 8,\n",
        "    hidden_dropout_prob = 0.2,\n",
        "    attention_probs_dropout_prob = 0.2,\n",
        "    ignore_mismatched_sizes=True,\n",
        "    bos_token_id = tokenizer.bos_token_id,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    eos_token_id = tokenizer.eos_token_id,\n",
        "    sep_token_id = tokenizer.sep_token_id\n",
        "    )\n",
        "\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\n",
        "    MODEL_TYPE,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "tokenizer.padding_side = \"left\"\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Send the model to the device.\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD12jNzBvss-"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "  lr = L_RATE, \n",
        "  eps = 1e-8\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4V5cmgjKNwA"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjY-4ld-ceo3",
        "outputId": "17ea0cc1-832f-4c7b-f94d-71896646a447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K06P9EM1DQoy"
      },
      "outputs": [],
      "source": [
        "# with open ('/content/drive/MyDrive/model.bin', 'rb') as fp:\n",
        "#   states = torch.load(fp)\n",
        "\n",
        "#   curr_epoch = states['epoch']\n",
        "#   model.load_state_dict(states['model'])\n",
        "#   optimizer.load_state_dict(states['optimizer'])\n",
        "#   fold_val_acc_list = states['losses']\n",
        "\n",
        "#   del states\n",
        "\n",
        "# curr_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQw7-PznxAXk"
      },
      "outputs": [],
      "source": [
        "# initial settings\n",
        "\n",
        "curr_epoch = 0\n",
        "\n",
        "# Store the accuracy scores for each fold model in this list.\n",
        "# [[model_0 scores], [model_1 scores], [model_2 scores], [model_3 scores], [model_4 scores]]\n",
        "# [[ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...]]\n",
        "\n",
        "# Create a list of lists to store the val acc results.\n",
        "# The number of items in this list will correspond to\n",
        "# the number of folds that the model is being trained on.\n",
        "fold_val_acc_list = []\n",
        "\n",
        "for i in range(0, NUM_FOLDS):\n",
        "    \n",
        "    # append an empty list\n",
        "    fold_val_acc_list.append([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899,
          "referenced_widgets": [
            "9b3f316114054a828a9316d6ea4f8317",
            "5682ab26874e4ae580c7742726b03bfb",
            "580d74269b3e4789af6fb2985b7b4bd6",
            "7eb0cd71a85f4cb39cd6dad26fc3f89d",
            "c31a3f9cd8344186832884eea5b358b1",
            "5aba6f4d28b949a8a9cfc638d54587c9",
            "44a00948b9d845ed9e7524ce46f28cb7",
            "444dd4840f934ba99a8b2d642215d015",
            "6db6ab219e3b480eaafd19cec8c273b9",
            "4c94cdb88fab49e8a50feea4a695dd98",
            "4812da0a540f45108ec8396730d26f50"
          ]
        },
        "id": "XibubGGrKNwA",
        "outputId": "ed9570e3-0c08-4cd8-e4c8-8f0db3441174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Num folds used for training: 5\n",
            "======== Epoch 1 / 100 ========\n",
            "\n",
            "== Fold Model 1\n",
            "Training...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b3f316114054a828a9316d6ea4f8317",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "import pickle\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set a seed value.\n",
        "seed_val = 1024\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "    \n",
        "\n",
        "# For each epoch...\n",
        "for epoch in range(curr_epoch, NUM_EPOCHS):\n",
        "    \n",
        "    print(\"\\nNum folds used for training:\", NUM_FOLDS_TO_TRAIN)\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n",
        "    \n",
        "    # Get the number of folds\n",
        "    num_folds = len(train_df_list)\n",
        "\n",
        "    # For this epoch, store the val acc scores for each fold in this list.\n",
        "    # We will use this list to calculate the cv at the end of the epoch.\n",
        "    epoch_acc_scores_list = []\n",
        "    \n",
        "    # For each fold...\n",
        "    for fold_index in range(1, NUM_FOLDS_TO_TRAIN):\n",
        "        \n",
        "        print('\\n== Fold Model', fold_index)\n",
        "\n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "        \n",
        "        stacked_val_labels = []\n",
        "        targets_list = []\n",
        "\n",
        "        print('Training...')\n",
        "\n",
        "        progress_bar_train =  tqdm(range(len(train_dataloader)))\n",
        "\n",
        "        # put the model into train mode\n",
        "        model.train()\n",
        "\n",
        "        # This turns gradient calculations on and off.\n",
        "        torch.set_grad_enabled(True)\n",
        "\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "            train_status = 'Batch ' + str(i+1) + ' of ' + str(len(train_dataloader))\n",
        "\n",
        "            print(train_status, end='\\r')\n",
        "\n",
        "\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_language_ids = batch[2].to(device)\n",
        "            b_labels = batch[3].to(device)\n",
        "\n",
        "            model.zero_grad()        \n",
        "\n",
        "\n",
        "            outputs = model(b_input_ids, \n",
        "                        attention_mask=b_input_mask,\n",
        "                        language_ids=b_language_ids,\n",
        "                        labels=b_labels)\n",
        "            \n",
        "            progress_bar_train.update(1)\n",
        "\n",
        "            # Get the loss from the outputs tuple: (loss, logits)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            # Convert the loss from a torch tensor to a number.\n",
        "            # Calculate the total loss.\n",
        "            total_train_loss = total_train_loss + loss.item()\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "            \n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Use the optimizer to update Weights\n",
        "            \n",
        "            # Optimizer for GPU\n",
        "            optimizer.step() \n",
        "            \n",
        "\n",
        "        print('Train loss:' ,total_train_loss)\n",
        "\n",
        "\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "\n",
        "        print('\\nValidation...')\n",
        "\n",
        "        progress_bar_eval =  tqdm(range(len(val_dataloader)))\n",
        "\n",
        "        # Put the model in evaluation mode.\n",
        "        model.eval()\n",
        "\n",
        "        # Turn off the gradient calculations.\n",
        "        # This tells the model not to compute or store gradients.\n",
        "        # This step saves memory and speeds up validation.\n",
        "        torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_val_loss = 0\n",
        "\n",
        "\n",
        "        for j, val_batch in enumerate(val_dataloader):\n",
        "\n",
        "            val_status = 'Batch ' + str(j+1) + ' of ' + str(len(val_dataloader))\n",
        "\n",
        "            print(val_status, end='\\r')\n",
        "\n",
        "            b_input_ids = val_batch[0].to(device)\n",
        "            b_input_mask = val_batch[1].to(device)\n",
        "            b_language_ids = batch[2].to(device)\n",
        "            b_labels = val_batch[3].to(device)\n",
        "\n",
        "\n",
        "            outputs = model(b_input_ids, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    language_ids=b_language_ids,\n",
        "                    labels=b_labels)\n",
        "            \n",
        "            progress_bar_eval.update(1)\n",
        "\n",
        "            # Get the loss from the outputs tuple: (loss, logits)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            # Convert the loss from a torch tensor to a number.\n",
        "            # Calculate the total loss.\n",
        "            total_val_loss = total_val_loss + loss.item()\n",
        "\n",
        "            # Get the preds\n",
        "            preds = outputs[1]\n",
        "\n",
        "\n",
        "            # Move preds to the CPU\n",
        "            val_preds = preds.detach().cpu().numpy()\n",
        "\n",
        "            # Move the labels to the cpu\n",
        "            targets_np = b_labels.to('cpu').numpy()\n",
        "\n",
        "            # Append the labels to a numpy list\n",
        "            targets_list.extend(targets_np)\n",
        "\n",
        "            if j == 0:  # first batch\n",
        "                stacked_val_preds = val_preds\n",
        "\n",
        "            else:\n",
        "                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
        "                \n",
        "                \n",
        "        # .........................................\n",
        "        # Calculate the val accuracy for this fold\n",
        "        # .........................................      \n",
        "\n",
        "\n",
        "        # Calculate the validation accuracy\n",
        "        y_true = targets_list\n",
        "        y_pred = np.argmax(stacked_val_preds, axis=1)\n",
        "\n",
        "        val_acc = accuracy_score(y_true, y_pred)\n",
        "        \n",
        "        \n",
        "        epoch_acc_scores_list.append(val_acc)\n",
        "\n",
        "\n",
        "        print('Val loss:' ,total_val_loss)\n",
        "        print('Val acc: ', val_acc)\n",
        "        \n",
        "        \n",
        "        # .........................\n",
        "        # Save the best model\n",
        "        # .........................\n",
        "        \n",
        "        if epoch == 0:\n",
        "            \n",
        "            # Save the Model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'losses': fold_val_acc_list\n",
        "            }, '/content/drive/MyDrive/model.bin')\n",
        "            print('Saved model.')\n",
        "            \n",
        "        if epoch != 0:\n",
        "        \n",
        "            val_acc_list = fold_val_acc_list[fold_index]\n",
        "            best_val_acc = max(val_acc_list)\n",
        "            \n",
        "            if val_acc > best_val_acc:\n",
        "                # save the model\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'losses': fold_val_acc_list\n",
        "                }, '/content/drive/MyDrive/model.bin')\n",
        "                print('Val acc improved. Saved model')\n",
        "                \n",
        "                \n",
        "        # .....................................\n",
        "        # Save the val_acc for this fold model\n",
        "        # .....................................\n",
        "        \n",
        "        # Note: Don't do this before the above 'Save Model' code or \n",
        "        # the save model code won't work. This is because the best_val_acc will\n",
        "        # become current val accuracy.\n",
        "                \n",
        "        # fold_val_acc_list is a list of lists.\n",
        "        # Each fold model has it's own list corresponding to the fold index.\n",
        "        # Here we choose a list corresponding to the fold number and append the acc score to that list.\n",
        "        fold_val_acc_list[fold_index].append(val_acc)\n",
        "        \n",
        "            \n",
        "\n",
        "        # Use the garbage collector to save memory.\n",
        "        gc.collect()\n",
        "        \n",
        "    # .............................................................\n",
        "    # Calculate the CV accuracy score over all folds in this epoch\n",
        "    # .............................................................   \n",
        "        \n",
        "        \n",
        "    # Print the average val accuracy for all 5 folds\n",
        "    cv_acc = sum(epoch_acc_scores_list)/NUM_FOLDS_TO_TRAIN\n",
        "    print(\"\\nCV Acc:\", cv_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YphTxiVPVw2Z",
        "outputId": "d55070c4-d5b7-4e9e-e82c-dd4320ea77d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b_input_mask[0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
    },
    "papermill": {
      "duration": 2795.593309,
      "end_time": "2020-08-19T07:29:53.573708",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-08-19T06:43:17.980399",
      "version": "2.1.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "e07e2cad301f10046f31ca6b8439b04dc67a22fe5bd747bca8a9458062e70f77"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "444dd4840f934ba99a8b2d642215d015": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44a00948b9d845ed9e7524ce46f28cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4812da0a540f45108ec8396730d26f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c94cdb88fab49e8a50feea4a695dd98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5682ab26874e4ae580c7742726b03bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aba6f4d28b949a8a9cfc638d54587c9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_44a00948b9d845ed9e7524ce46f28cb7",
            "value": "  0%"
          }
        },
        "580d74269b3e4789af6fb2985b7b4bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_444dd4840f934ba99a8b2d642215d015",
            "max": 1500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6db6ab219e3b480eaafd19cec8c273b9",
            "value": 0
          }
        },
        "5aba6f4d28b949a8a9cfc638d54587c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6db6ab219e3b480eaafd19cec8c273b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7eb0cd71a85f4cb39cd6dad26fc3f89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c94cdb88fab49e8a50feea4a695dd98",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4812da0a540f45108ec8396730d26f50",
            "value": " 0/1500 [00:00&lt;?, ?it/s]"
          }
        },
        "9b3f316114054a828a9316d6ea4f8317": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5682ab26874e4ae580c7742726b03bfb",
              "IPY_MODEL_580d74269b3e4789af6fb2985b7b4bd6",
              "IPY_MODEL_7eb0cd71a85f4cb39cd6dad26fc3f89d"
            ],
            "layout": "IPY_MODEL_c31a3f9cd8344186832884eea5b358b1"
          }
        },
        "c31a3f9cd8344186832884eea5b358b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
