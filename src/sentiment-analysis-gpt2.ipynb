{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rHWZiZrKNvr",
        "outputId": "927204aa-0316-42b9-ba33-1ddb9a5c3d63"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# set a seed value\n",
        "torch.manual_seed(555)\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfnrNVRPKNv5"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpGoTFB1KNv5"
      },
      "source": [
        "## 2.1. Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc49LdYRa0oj"
      },
      "outputs": [],
      "source": [
        "sentiment_to_num = {\n",
        "    'Negative': 0,\n",
        "    'Neutral': 1,\n",
        "    'Positive': 2\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "KOW0AbsGYThF",
        "outputId": "789491d5-f4b1-4eca-c02b-729ff90064af"
      },
      "outputs": [],
      "source": [
        "sampled_df = pd.read_csv(r'sampled_data.csv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "df_train = sampled_df.groupby('sentiment').sample(n=40000).sample(frac=1)\n",
        "df_test = sampled_df.filter(items=list(filter(lambda x: x not in df_train.index, sampled_df.index)), axis=0)\n",
        "df_train = df_train.reset_index()[['text', 'sentiment']]\n",
        "df_test = df_test.reset_index()[['text', 'sentiment']]\n",
        "df_train['sentiment'] =  df_train['sentiment'].apply(lambda x: sentiment_to_num[x])\n",
        "df_test['sentiment'] =  df_test['sentiment'].apply(lambda x: sentiment_to_num[x])\n",
        "df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JIa0mAkKNv6"
      },
      "source": [
        "## 2.2. Create Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCR804L3KNv6",
        "outputId": "c090222a-b1f9-48a6-c66a-82b241991042"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "# shuffle\n",
        "df = shuffle(df_train)\n",
        "\n",
        "# initialize kfold\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1024)\n",
        "\n",
        "# for stratification\n",
        "y = df['sentiment']\n",
        "\n",
        "# Note:\n",
        "# Each fold is a tuple ([train_index_values], [val_index_values])\n",
        "# fold_0, fold_1, fold_2, fold_3, fold_5 = kf.split(df, y)\n",
        "\n",
        "# Put the folds into a list. This is a list of tuples.\n",
        "fold_list = list(kf.split(df, y))\n",
        "\n",
        "train_df_list = []\n",
        "val_df_list = []\n",
        "\n",
        "for i, fold in enumerate(fold_list):\n",
        "\n",
        "    # map the train and val index values to dataframe rows\n",
        "    df_train = df[df.index.isin(fold[0])]\n",
        "    df_val = df[df.index.isin(fold[1])]\n",
        "    \n",
        "    train_df_list.append(df_train)\n",
        "    val_df_list.append(df_val)\n",
        "    \n",
        "    \n",
        "\n",
        "print(len(train_df_list))\n",
        "print(len(val_df_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67FLvqOoKNv7"
      },
      "source": [
        "# Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzoOOWhnKNv7"
      },
      "source": [
        "## 3.1. Train a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo6HRmYJKNv8",
        "outputId": "7a30f281-cfa6-47b2-f969-31e487684875"
      },
      "outputs": [],
      "source": [
        "MODEL_TYPE = 'gpt2'\n",
        "\n",
        "NUM_FOLDS = 5\n",
        "\n",
        "# Saving 5 TPU models will exceed the 4.9GB disk space.\n",
        "# Therefore, will will only train on 3 folds.\n",
        "NUM_FOLDS_TO_TRAIN = 5 \n",
        "\n",
        "L_RATE = 2e-5\n",
        "MAX_LEN = 256\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "NUM_CORES = os.cpu_count()\n",
        "\n",
        "NUM_CORES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9I9XQ05KNv8"
      },
      "source": [
        "## Instantiate the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxOCRznb1mBz"
      },
      "outputs": [],
      "source": [
        "token_dict = {\n",
        "    'bos_token': '<|beginoftext|>',\n",
        "    'pad_token': '<|pad|>',\n",
        "    'sep_token': '<|sep|>',\n",
        "    'mask_token': '<|mask|>'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGijUa_wKNv8",
        "outputId": "97be4d74-113b-4314-c5b4-8c1c3db1ae94"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load the tokenizer.\n",
        "print('Loading tokenizer...')\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(MODEL_TYPE, do_lower_case=True)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"tokenizer-gpt2/\", do_lower_case=True)\n",
        "tokenizer.add_special_tokens(token_dict)\n",
        "print(len(set(tokenizer.get_vocab().keys())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg9uCaLfiuB8"
      },
      "outputs": [],
      "source": [
        "# add vocabulary to tokenizer\n",
        "# ===========================\n",
        "\n",
        "# from tqdm.auto import tqdm\n",
        "# import nltk\n",
        "# nltk.download('words')\n",
        "\n",
        "# with open('combined-malay-dict.txt', encoding=\"utf8\") as fp:\n",
        "#   malay_words = set([x.strip() for x in fp.readlines()])\n",
        "\n",
        "# english_words = set(nltk.corpus.words.words())\n",
        "\n",
        "# new_tokens = (malay_words | english_words) - set(tokenizer.get_vocab().keys())\n",
        "\n",
        "# new_tokens = list(new_tokens)\n",
        "# batchsize = 10000\n",
        "# progress_bar =  tqdm(range(len(new_tokens)))\n",
        "\n",
        "# for i in range(0, len(new_tokens), batchsize):\n",
        "#   batch = new_tokens[i:i+batchsize]\n",
        "#   tokenizer.add_tokens(batch)\n",
        "#   progress_bar.update(batchsize)\n",
        "\n",
        "# tokenizer.add_special_tokens(token_dict)\n",
        "# tokenizer.save_pretrained(\"tokenizer/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkknBn2sKNv8"
      },
      "source": [
        "## Create the Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxbAT37U7w4m",
        "outputId": "84cf7eb7-ac99-4c47-d565-a6fe331c91b0"
      },
      "outputs": [],
      "source": [
        "from language_tokens import get_lang_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEyvCmKbKNv8"
      },
      "outputs": [],
      "source": [
        "class CompDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df_data = df\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # get the sentence from the dataframe\n",
        "        features = self.df_data.loc[index, 'text']\n",
        "\n",
        "        # Process the sentence\n",
        "        # ---------------------\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                    features,           # Sentences to encode.\n",
        "                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
        "                    truncation = True,\n",
        "                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n",
        "                    pad_to_max_length = True,\n",
        "                    return_attention_mask = True,   # Construct attn. masks.\n",
        "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
        "               )  \n",
        "        \n",
        "        # These are torch tensors already.\n",
        "        input_ids = encoded_dict['input_ids'][0]\n",
        "        att_mask = encoded_dict['attention_mask'][0]\n",
        "        language_ids = torch.tensor(get_lang_tokens(\n",
        "            [x.replace(' ', '') for x in tokenizer.batch_decode(input_ids.tolist())]\n",
        "            ))\n",
        "        \n",
        "        # Convert the target to a torch tensor\n",
        "        target = torch.tensor(self.df_data.loc[index, 'sentiment'])\n",
        "\n",
        "        sample = (input_ids, att_mask, language_ids, target)\n",
        "\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "class TestDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df_data = df\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # get the sentence from the dataframe\n",
        "        features = self.df_data.loc[index, 'text']\n",
        "\n",
        "        # Process the sentence\n",
        "        # ---------------------\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                    features,           # Sentence to encode.\n",
        "                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
        "                    truncation = True,\n",
        "                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n",
        "                    pad_to_max_length = True,\n",
        "                    return_attention_mask = True,   # Construct attn. masks.\n",
        "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
        "               )\n",
        "        \n",
        "        # These are torch tensors already.\n",
        "        input_ids = encoded_dict['input_ids'][0]\n",
        "        att_mask = encoded_dict['attention_mask'][0]\n",
        "        language_ids = torch.tensor(get_lang_tokens(\n",
        "            [x.replace(' ', '') for x in tokenizer.batch_decode(input_ids.tolist())]\n",
        "            ))\n",
        "               \n",
        "\n",
        "        sample = (input_ids, att_mask, language_ids)\n",
        "\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wicbcrP7KNv9"
      },
      "source": [
        "## Test the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "cwzDjEw2KNv9",
        "outputId": "1006e1a0-8dc1-46a9-d761-f8d073381fc7"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.reset_index(drop=True)\n",
        "df_val = df_val.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcVcVZ2ZKNv9",
        "outputId": "db54ee37-0ef2-4768-9efc-4bee11f540c8"
      },
      "outputs": [],
      "source": [
        "train_data = CompDataset(df_train)\n",
        "val_data = CompDataset(df_val)\n",
        "test_data = TestDataset(df_test)\n",
        "\n",
        "\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        # shuffle=True,\n",
        "                                       num_workers=NUM_CORES)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(val_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True,\n",
        "                                       num_workers=NUM_CORES)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=False,\n",
        "                                       num_workers=NUM_CORES)\n",
        "\n",
        "\n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(val_dataloader))\n",
        "print(len(test_dataloader))\n",
        "\n",
        "input_ids, att_mask, language_ids, target = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qQUwQYC9ZKFf",
        "outputId": "c137f4d2-b4a3-4765-ee9f-8f89adc620a7"
      },
      "outputs": [],
      "source": [
        "for i, b in enumerate(train_dataloader):\n",
        "  print(i, len(b), b[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFqVMqhzZfvX",
        "outputId": "02245695-9071-4802-d3c3-b0e711a80400"
      },
      "outputs": [],
      "source": [
        "[x.shape for x in train_data[311]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iwctngbKNv9"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3VkN5AMRWBt",
        "outputId": "0cd9acdf-5783-440f-c400-ad957c8d0343"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# check if we have cuda installed\n",
        "if torch.cuda.is_available():\n",
        "    # to use GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('GPU is:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx9FDeWZKNv9",
        "outputId": "7d37e156-17a4-47f0-9ee7-031fb64c7887"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Config\n",
        "from modeling_gpt2 import GPT2ForSequenceClassification\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "config = GPT2Config.from_pretrained(\n",
        "    MODEL_TYPE,\n",
        "    num_labels = len(set([x.item() for x in target])) ,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "    num_hidden_layers = 5,\n",
        "    num_attention_heads = 8,\n",
        "    hidden_dropout_prob = 0.2,\n",
        "    attention_probs_dropout_prob = 0.2,\n",
        "    ignore_mismatched_sizes=True,\n",
        "    bos_token_id = tokenizer.bos_token_id,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    eos_token_id = tokenizer.eos_token_id,\n",
        "    sep_token_id = tokenizer.sep_token_id\n",
        "    )\n",
        "\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\n",
        "    MODEL_TYPE,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "tokenizer.padding_side = \"left\"\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Send the model to the device.\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD12jNzBvss-"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "  lr = L_RATE, \n",
        "  eps = 1e-8\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4V5cmgjKNwA"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjY-4ld-ceo3",
        "outputId": "17ea0cc1-832f-4c7b-f94d-71896646a447"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K06P9EM1DQoy"
      },
      "outputs": [],
      "source": [
        "# with open ('/content/drive/MyDrive/model.bin', 'rb') as fp:\n",
        "#   states = torch.load(fp)\n",
        "\n",
        "#   curr_epoch = states['epoch']\n",
        "#   model.load_state_dict(states['model'])\n",
        "#   optimizer.load_state_dict(states['optimizer'])\n",
        "#   fold_val_acc_list = states['losses']\n",
        "\n",
        "#   del states\n",
        "\n",
        "# curr_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQw7-PznxAXk"
      },
      "outputs": [],
      "source": [
        "# initial settings\n",
        "\n",
        "curr_epoch = 0\n",
        "\n",
        "# Store the accuracy scores for each fold model in this list.\n",
        "# [[model_0 scores], [model_1 scores], [model_2 scores], [model_3 scores], [model_4 scores]]\n",
        "# [[ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...]]\n",
        "\n",
        "# Create a list of lists to store the val acc results.\n",
        "# The number of items in this list will correspond to\n",
        "# the number of folds that the model is being trained on.\n",
        "fold_val_acc_list = []\n",
        "\n",
        "for i in range(0, NUM_FOLDS):\n",
        "    \n",
        "    # append an empty list\n",
        "    fold_val_acc_list.append([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899,
          "referenced_widgets": [
            "9b3f316114054a828a9316d6ea4f8317",
            "5682ab26874e4ae580c7742726b03bfb",
            "580d74269b3e4789af6fb2985b7b4bd6",
            "7eb0cd71a85f4cb39cd6dad26fc3f89d",
            "c31a3f9cd8344186832884eea5b358b1",
            "5aba6f4d28b949a8a9cfc638d54587c9",
            "44a00948b9d845ed9e7524ce46f28cb7",
            "444dd4840f934ba99a8b2d642215d015",
            "6db6ab219e3b480eaafd19cec8c273b9",
            "4c94cdb88fab49e8a50feea4a695dd98",
            "4812da0a540f45108ec8396730d26f50"
          ]
        },
        "id": "XibubGGrKNwA",
        "outputId": "ed9570e3-0c08-4cd8-e4c8-8f0db3441174"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "import pickle\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set a seed value.\n",
        "seed_val = 1024\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "    \n",
        "\n",
        "# For each epoch...\n",
        "for epoch in range(curr_epoch, NUM_EPOCHS):\n",
        "    \n",
        "    print(\"\\nNum folds used for training:\", NUM_FOLDS_TO_TRAIN)\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n",
        "    \n",
        "    # Get the number of folds\n",
        "    num_folds = len(train_df_list)\n",
        "\n",
        "    # For this epoch, store the val acc scores for each fold in this list.\n",
        "    # We will use this list to calculate the cv at the end of the epoch.\n",
        "    epoch_acc_scores_list = []\n",
        "    \n",
        "    # For each fold...\n",
        "    for fold_index in range(1, NUM_FOLDS_TO_TRAIN):\n",
        "        \n",
        "        print('\\n== Fold Model', fold_index)\n",
        "\n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "        \n",
        "        stacked_val_labels = []\n",
        "        targets_list = []\n",
        "\n",
        "        print('Training...')\n",
        "\n",
        "        progress_bar_train =  tqdm(range(len(train_dataloader)))\n",
        "\n",
        "        # put the model into train mode\n",
        "        model.train()\n",
        "\n",
        "        # This turns gradient calculations on and off.\n",
        "        torch.set_grad_enabled(True)\n",
        "\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "            train_status = 'Batch ' + str(i+1) + ' of ' + str(len(train_dataloader))\n",
        "\n",
        "            print(train_status, end='\\r')\n",
        "\n",
        "\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_language_ids = batch[2].to(device)\n",
        "            b_labels = batch[3].to(device)\n",
        "\n",
        "            model.zero_grad()        \n",
        "\n",
        "\n",
        "            outputs = model(b_input_ids, \n",
        "                        attention_mask=b_input_mask,\n",
        "                        language_ids=b_language_ids,\n",
        "                        labels=b_labels)\n",
        "            \n",
        "            progress_bar_train.update(1)\n",
        "\n",
        "            # Get the loss from the outputs tuple: (loss, logits)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            # Convert the loss from a torch tensor to a number.\n",
        "            # Calculate the total loss.\n",
        "            total_train_loss = total_train_loss + loss.item()\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "            \n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Use the optimizer to update Weights\n",
        "            \n",
        "            # Optimizer for GPU\n",
        "            optimizer.step() \n",
        "            \n",
        "\n",
        "        print('Train loss:' ,total_train_loss)\n",
        "\n",
        "\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "\n",
        "        print('\\nValidation...')\n",
        "\n",
        "        progress_bar_eval =  tqdm(range(len(val_dataloader)))\n",
        "\n",
        "        # Put the model in evaluation mode.\n",
        "        model.eval()\n",
        "\n",
        "        # Turn off the gradient calculations.\n",
        "        # This tells the model not to compute or store gradients.\n",
        "        # This step saves memory and speeds up validation.\n",
        "        torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_val_loss = 0\n",
        "\n",
        "\n",
        "        for j, val_batch in enumerate(val_dataloader):\n",
        "\n",
        "            val_status = 'Batch ' + str(j+1) + ' of ' + str(len(val_dataloader))\n",
        "\n",
        "            print(val_status, end='\\r')\n",
        "\n",
        "            b_input_ids = val_batch[0].to(device)\n",
        "            b_input_mask = val_batch[1].to(device)\n",
        "            b_language_ids = batch[2].to(device)\n",
        "            b_labels = val_batch[3].to(device)\n",
        "\n",
        "\n",
        "            outputs = model(b_input_ids, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    language_ids=b_language_ids,\n",
        "                    labels=b_labels)\n",
        "            \n",
        "            progress_bar_eval.update(1)\n",
        "\n",
        "            # Get the loss from the outputs tuple: (loss, logits)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            # Convert the loss from a torch tensor to a number.\n",
        "            # Calculate the total loss.\n",
        "            total_val_loss = total_val_loss + loss.item()\n",
        "\n",
        "            # Get the preds\n",
        "            preds = outputs[1]\n",
        "\n",
        "\n",
        "            # Move preds to the CPU\n",
        "            val_preds = preds.detach().cpu().numpy()\n",
        "\n",
        "            # Move the labels to the cpu\n",
        "            targets_np = b_labels.to('cpu').numpy()\n",
        "\n",
        "            # Append the labels to a numpy list\n",
        "            targets_list.extend(targets_np)\n",
        "\n",
        "            if j == 0:  # first batch\n",
        "                stacked_val_preds = val_preds\n",
        "\n",
        "            else:\n",
        "                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
        "                \n",
        "                \n",
        "        # .........................................\n",
        "        # Calculate the val accuracy for this fold\n",
        "        # .........................................      \n",
        "\n",
        "\n",
        "        # Calculate the validation accuracy\n",
        "        y_true = targets_list\n",
        "        y_pred = np.argmax(stacked_val_preds, axis=1)\n",
        "\n",
        "        val_acc = accuracy_score(y_true, y_pred)\n",
        "        \n",
        "        \n",
        "        epoch_acc_scores_list.append(val_acc)\n",
        "\n",
        "\n",
        "        print('Val loss:' ,total_val_loss)\n",
        "        print('Val acc: ', val_acc)\n",
        "        \n",
        "        \n",
        "        # .........................\n",
        "        # Save the best model\n",
        "        # .........................\n",
        "        \n",
        "        if epoch == 0:\n",
        "            \n",
        "            # Save the Model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'losses': fold_val_acc_list\n",
        "            }, '/content/drive/MyDrive/model.bin')\n",
        "            print('Saved model.')\n",
        "            \n",
        "        if epoch != 0:\n",
        "        \n",
        "            val_acc_list = fold_val_acc_list[fold_index]\n",
        "            best_val_acc = max(val_acc_list)\n",
        "            \n",
        "            if val_acc > best_val_acc:\n",
        "                # save the model\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'losses': fold_val_acc_list\n",
        "                }, '/content/drive/MyDrive/model.bin')\n",
        "                print('Val acc improved. Saved model')\n",
        "                \n",
        "                \n",
        "        # .....................................\n",
        "        # Save the val_acc for this fold model\n",
        "        # .....................................\n",
        "        \n",
        "        # Note: Don't do this before the above 'Save Model' code or \n",
        "        # the save model code won't work. This is because the best_val_acc will\n",
        "        # become current val accuracy.\n",
        "                \n",
        "        # fold_val_acc_list is a list of lists.\n",
        "        # Each fold model has it's own list corresponding to the fold index.\n",
        "        # Here we choose a list corresponding to the fold number and append the acc score to that list.\n",
        "        fold_val_acc_list[fold_index].append(val_acc)\n",
        "        \n",
        "            \n",
        "\n",
        "        # Use the garbage collector to save memory.\n",
        "        gc.collect()\n",
        "        \n",
        "    # .............................................................\n",
        "    # Calculate the CV accuracy score over all folds in this epoch\n",
        "    # .............................................................   \n",
        "        \n",
        "        \n",
        "    # Print the average val accuracy for all 5 folds\n",
        "    cv_acc = sum(epoch_acc_scores_list)/NUM_FOLDS_TO_TRAIN\n",
        "    print(\"\\nCV Acc:\", cv_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YphTxiVPVw2Z",
        "outputId": "d55070c4-d5b7-4e9e-e82c-dd4320ea77d3"
      },
      "outputs": [],
      "source": [
        "b_input_mask[0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
    },
    "papermill": {
      "duration": 2795.593309,
      "end_time": "2020-08-19T07:29:53.573708",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-08-19T06:43:17.980399",
      "version": "2.1.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "e07e2cad301f10046f31ca6b8439b04dc67a22fe5bd747bca8a9458062e70f77"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "444dd4840f934ba99a8b2d642215d015": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44a00948b9d845ed9e7524ce46f28cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4812da0a540f45108ec8396730d26f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c94cdb88fab49e8a50feea4a695dd98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5682ab26874e4ae580c7742726b03bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aba6f4d28b949a8a9cfc638d54587c9",
            "placeholder": "​",
            "style": "IPY_MODEL_44a00948b9d845ed9e7524ce46f28cb7",
            "value": "  0%"
          }
        },
        "580d74269b3e4789af6fb2985b7b4bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_444dd4840f934ba99a8b2d642215d015",
            "max": 1500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6db6ab219e3b480eaafd19cec8c273b9",
            "value": 0
          }
        },
        "5aba6f4d28b949a8a9cfc638d54587c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6db6ab219e3b480eaafd19cec8c273b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7eb0cd71a85f4cb39cd6dad26fc3f89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c94cdb88fab49e8a50feea4a695dd98",
            "placeholder": "​",
            "style": "IPY_MODEL_4812da0a540f45108ec8396730d26f50",
            "value": " 0/1500 [00:00&lt;?, ?it/s]"
          }
        },
        "9b3f316114054a828a9316d6ea4f8317": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5682ab26874e4ae580c7742726b03bfb",
              "IPY_MODEL_580d74269b3e4789af6fb2985b7b4bd6",
              "IPY_MODEL_7eb0cd71a85f4cb39cd6dad26fc3f89d"
            ],
            "layout": "IPY_MODEL_c31a3f9cd8344186832884eea5b358b1"
          }
        },
        "c31a3f9cd8344186832884eea5b358b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
